[
  {
    "path": "posts/2021-05-22-contextual-importance-and-contextual-utility/",
    "title": "Explaining Machine Learning Models Using Contextual Importance and Contextual Utility",
    "description": "A machine learning explanability algorithm",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2021-11-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nWhat Kinds of explanation does CIU generate?\r\nHow does CIU work?\r\nA toy example: predicting breast\r\nPermutation feature importance\r\nDecision Tree Classifier\r\nRandom Forest Classifier\r\nGradient Boosting Classifier\r\nExplaining a single observation\r\nGenerating Textual Explanations\r\nDrawbacks\r\n\r\n\r\nIntroduction\r\nExplainability is a hot topic in the machine learning research community these days. Over the past few years, many methods have been introduced to understand how a machine learning model makes a prediction. However, explainability is not an entirely new concept, and it was actually started a few decades ago. In this blog post, I will introduce a rather unknown but simple technique that was introduced almost 20 years ago. This technique is called Contextual Importance and Utility (CIU) for explaining ML models and show you how we can explain any types of machine learning. This method relies on the notion of context is important.\r\nFor example, imagine we are trying to predict house prices from a set of features such as the number of bedrooms and pools. If every house in the dataset has no pool (the current context), then the feature corresponding to it has no usefulness and no importance for predicting a model. On the other hand, in a city where most houses have one or two bedrooms (again the current context), houses with three or more bedrooms are more unusual.\r\nWhat Kinds of explanation does CIU generate?\r\nIt is a model-agnostic methods, and it can explain the output of any “black-box” machine learning model.\r\nIt produces local explanations, which means that the explanations are generated for individual instances (not the whole model), and they show which features are more important for an individual observation.\r\nIt gives us post-hoc explanations as it is a method that processes the output of a machine learning model after training.\r\nUnlike LIME and many other techniques, CIU does not approximate or transforms what a model predicts but instead directly explain predictions. It can also provide a contrastive explanation. For instance, why did the model predict rainy and not cloudy?\r\nHow does CIU work?\r\nCIT estimates two values that aim to explain the context in which a machine learning model predicts:\r\nContextual Importance (CI) measures how much change in the range and output values can be attributed to one (or several) input variables. CU is based on the notion that a variable which results in a broader ranger of output values would be more critical. Formally, CIU is defined as follows:\r\nCI = (Cmax - Cmin)/(absmax - absmin)\r\nContextual Utility (CU) indicates how favorable the current value of one (or several) input variables is for a high output value. CU is computed using the following formula:\r\nCU = (out - Cmin)/(Cmax - Cmin)\r\nCmax and Cmin are the highest and lowest values that the output of an ML model can take by changing the input feature(s). Obtaining Cmax and Cmin is computationally, and mathematically is not a trivial task. In the original paper, these values are computed using a Monte Carlo simulation, where a lot of observations were generated. Also, absmax and absmin indicate the absolute range of values that the output has taken. For example, In classification problems, the absolute minimum and maximum range of values are the predicted probabilities of machine learning models between 0 and 1.\r\nCIU is implemented both in python and R. For simplicity, I will use its python implementation (py-ciu library) in this blogpost.\r\nYou can install py-ciu using the pip command:\r\n\r\n\r\nShow code\r\npip install py-ciu\r\n\r\nA toy example: predicting breast\r\nI will use the breast cancer dataset in scikit-learn to show how we can use CIU. I will train three different machine learning models, including a decision tree, a random forest, and a gradient boosting algorithm on this dataset and compute CI and CU values for a single instance from the test dataset.\r\nFirst, we need to load the necessary libraries and modules.\r\n\r\n\r\nShow code\r\nfrom ciu import determine_ciu\r\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\r\nfrom sklearn.inspection import permutation_importance\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n# for reproducability\r\nnp.random.seed(123)\r\n\r\nThen we split the dataset into a training and test set. We train our machine learning models on the training dataset and evaluate their performance on the test dataset. Note that for explaining ML models, we should use samples from the test dataset and not the training dataset.\r\n\r\n\r\nShow code\r\nX = pd.DataFrame(load_breast_cancer()['data'])\r\ny = load_breast_cancer()['target']\r\nX.columns = load_breast_cancer()['feature_names']\r\n\r\n\r\n\r\nShow code\r\nX_train,X_test, y_train,y_test = train_test_split(X,y,stratify = y)\r\n\r\n\r\n\r\nShow code\r\ndef fit_evaluate_model(clf):\r\n  clf = clf.fit(X_train, y_train)\r\n  print(' Accuracy on test dataset {}'.format(clf.score(X_test,y_test)))\r\n  return clf\r\n\r\nPermutation feature importance\r\nAs mentioned before, CIU only generates local explanations and does not give us a global overview of how a model makes a prediction. To gain a better understanding of the global importance of the model, we can compute the permutation feature importance scores:\r\n\r\n\r\nShow code\r\ndef print_permutation_importance(model):\r\n  imp_features = []\r\n  pi = permutation_importance(model, X_test, y_test,\r\n                            n_repeats=30,\r\n                           random_state=0)\r\n  for i in pi.importances_mean.argsort()[::-1]:\r\n       if pi.importances_mean[i] - 2 * pi.importances_std[i] > 0:\r\n           print(f\"{X_test.columns[i]:<8} \"\r\n                 f\"{pi.importances_mean[i]:.3f} \"\r\n                 f\" +/- {pi.importances_std[i]:.3f}\")\r\n           imp_features.append(pi.importances_mean[i])\r\n           if len(imp_features) == 0:\r\n                print('no important features')\r\n\r\nDecision Tree Classifier\r\nSince we just used a toy example, I will not be very picky about my model’s hyper-parameters and leave them to be the default values in sklearn.\r\n\r\n\r\nShow code\r\ndt = DecisionTreeClassifier()\r\ndt_fit = fit_evaluate_model(dt)\r\n Accuracy on test dataset 0.9370629370629371\r\n\r\n\r\n\r\nShow code\r\nprint_permutation_importance(dt_fit)\r\nworst perimeter 0.173  +/- 0.019\r\nworst concave points 0.145  +/- 0.023\r\nworst concavity 0.135  +/- 0.017\r\nworst area 0.063  +/- 0.014\r\nradius error 0.036  +/- 0.014\r\nworst smoothness 0.018  +/- 0.008\r\nmean area 0.017  +/- 0.006\r\n\r\nRandom Forest Classifier\r\n\r\n\r\nShow code\r\nrf = RandomForestClassifier(\r\n)\r\nrf_fit = fit_evaluate_model(rf)\r\n Accuracy on test dataset 0.972027972027972\r\n\r\n\r\n\r\nShow code\r\nprint_permutation_importance(rf_fit)\r\nworst texture 0.023  +/- 0.004\r\nmean texture 0.013  +/- 0.006\r\nworst smoothness 0.010  +/- 0.004\r\nmean concavity 0.010  +/- 0.005\r\nworst fractal dimension 0.006  +/- 0.003\r\n\r\nGradient Boosting Classifier\r\n\r\n\r\nShow code\r\ngb = GradientBoostingClassifier()\r\ngb_fit = fit_evaluate_model(gb)\r\n Accuracy on test dataset 0.9790209790209791\r\n\r\n\r\n\r\nShow code\r\nprint_permutation_importance(gb_fit)\r\nworst concave points 0.023  +/- 0.011\r\nmean concave points 0.021  +/- 0.010\r\n\r\nThe random forest and gradient boosting classifiers have the same accuracy score; however, their most important features are different.\r\nExplaining a single observation\r\nNow let us explain how each model predicts a single example (observation) from the test dataset.\r\n\r\n\r\nShow code\r\nexample = X_test.iloc[1,:]\r\nexample_prediction = gb.predict(example.values.reshape(1, -1))\r\nexample_prediction_prob = gb.predict_proba(example.values.reshape(1, -1))\r\nprediction_index = 0 if example_prediction > 0.5 else 1\r\nprint(f'Prediction {example_prediction}; Probability: {example_prediction_prob}')\r\nPrediction [1]; Probability: [[0.10952357 0.89047643]]\r\n\r\nTo obtain a CIU score, we need to compute the minimum and maximum observed value of each feature in the dataset.\r\n\r\n\r\nShow code\r\ndef min_max_features(X_train):\r\n  min_max = dict()\r\n  for i in range(len(X_train.columns)):\r\n      min_max[X_train.columns[i]] =[X_train.iloc[:,i].min(),X_train.iloc[:,i].max(),False]\r\n  return min_max\r\n  \r\nmin_max = min_max_features(X_train)\r\n\r\n\r\n\r\nShow code\r\ndef explain_ciu(example,model):\r\n  ciu = determine_ciu(\r\n      example.to_dict(),\r\n      model.predict_proba,\r\n      min_max,\r\n      1000,\r\n      prediction_index,\r\n  )\r\n  return ciu\r\n\r\n\r\n\r\nShow code\r\ndt_ciu = explain_ciu(example,dt_fit)\r\nrf_ciu = explain_ciu(example,rf_fit)\r\ngb_ciu = explain_ciu(example,gb_fit)\r\n\r\nGenerating Textual Explanations\r\nWe can obtain a textual explanation of CIU which indicates which feature(s) can be important for our test example\r\n\r\n\r\nShow code\r\ndt_ciu.text_explain()\r\n['The feature \"mean radius\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean texture\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean perimeter\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean area\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean smoothness\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean compactness\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean concavity\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean concave points\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean symmetry\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean fractal dimension\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"radius error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"texture error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"perimeter error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"area error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"smoothness error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"compactness error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"concavity error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"concave points error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"symmetry error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"fractal dimension error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst radius\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst texture\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst perimeter\", which is highly important (CI=100.0%), is very typical for its class (CU=100.0%).', 'The feature \"worst area\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst smoothness\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst compactness\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst concavity\", which is highly important (CI=100.0%), is very typical for its class (CU=100.0%).', 'The feature \"worst concave points\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst symmetry\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst fractal dimension\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).']\r\n\r\n\r\n\r\nShow code\r\nrf_ciu.text_explain()\r\n['The feature \"mean radius\", which is important (CI=32.26%), is very typical for its class (CU=90.0%).', 'The feature \"mean texture\", which is important (CI=35.48%), is unlikely for its class (CU=27.27%).', 'The feature \"mean perimeter\", which is not important (CI=12.9%), is typical for its class (CU=50.0%).', 'The feature \"mean area\", which is not important (CI=16.13%), is unlikely for its class (CU=40.0%).', 'The feature \"mean smoothness\", which is not important (CI=12.9%), is typical for its class (CU=50.0%).', 'The feature \"mean compactness\", which is not important (CI=9.68%), is unlikely for its class (CU=33.33%).', 'The feature \"mean concavity\", which is not important (CI=16.13%), is not typical for its class (CU=20.0%).', 'The feature \"mean concave points\", which is not important (CI=19.35%), is not typical for its class (CU=16.67%).', 'The feature \"mean symmetry\", which is important (CI=38.71%), is very typical for its class (CU=100.0%).', 'The feature \"mean fractal dimension\", which is not important (CI=6.45%), is not typical for its class (CU=0.1%).', 'The feature \"radius error\", which is not important (CI=22.58%), is typical for its class (CU=71.43%).', 'The feature \"texture error\", which is not important (CI=22.58%), is very typical for its class (CU=85.71%).', 'The feature \"perimeter error\", which is not important (CI=22.58%), is unlikely for its class (CU=42.86%).', 'The feature \"area error\", which is important (CI=38.71%), is unlikely for its class (CU=33.33%).', 'The feature \"smoothness error\", which is not important (CI=3.23%), is very typical for its class (CU=100.0%).', 'The feature \"compactness error\", which is not important (CI=12.9%), is typical for its class (CU=50.0%).', 'The feature \"concavity error\", which is not important (CI=6.45%), is very typical for its class (CU=100.0%).', 'The feature \"concave points error\", which is not important (CI=9.68%), is typical for its class (CU=66.67%).', 'The feature \"symmetry error\", which is not important (CI=6.45%), is typical for its class (CU=50.0%).', 'The feature \"fractal dimension error\", which is not important (CI=16.13%), is very typical for its class (CU=100.0%).', 'The feature \"worst radius\", which is very important (CI=51.61%), is very typical for its class (CU=87.5%).', 'The feature \"worst texture\", which is very important (CI=67.74%), is unlikely for its class (CU=33.33%).', 'The feature \"worst perimeter\", which is very important (CI=70.97%), is typical for its class (CU=63.64%).', 'The feature \"worst area\", which is very important (CI=61.29%), is typical for its class (CU=57.89%).', 'The feature \"worst smoothness\", which is not important (CI=6.45%), is typical for its class (CU=50.0%).', 'The feature \"worst compactness\", which is not important (CI=9.68%), is unlikely for its class (CU=33.33%).', 'The feature \"worst concavity\", which is very important (CI=64.52%), is very typical for its class (CU=85.0%).', 'The feature \"worst concave points\", which is important (CI=38.71%), is not typical for its class (CU=16.67%).', 'The feature \"worst symmetry\", which is important (CI=25.81%), is typical for its class (CU=50.0%).', 'The feature \"worst fractal dimension\", which is not important (CI=3.23%), is not typical for its class (CU=0.1%).']\r\n\r\n\r\n\r\nShow code\r\ngb_ciu.text_explain()\r\n['The feature \"mean radius\", which is not important (CI=16.49%), is not typical for its class (CU=0.65%).', 'The feature \"mean texture\", which is highly important (CI=90.14%), is not typical for its class (CU=3.76%).', 'The feature \"mean perimeter\", which is not important (CI=2.63%), is not typical for its class (CU=0.1%).', 'The feature \"mean area\", which is not important (CI=3.36%), is very typical for its class (CU=100.0%).', 'The feature \"mean smoothness\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"mean compactness\", which is important (CI=37.26%), is not typical for its class (CU=0.1%).', 'The feature \"mean concavity\", which is not important (CI=4.0%), is not typical for its class (CU=8.92%).', 'The feature \"mean concave points\", which is important (CI=38.25%), is not typical for its class (CU=3.57%).', 'The feature \"mean symmetry\", which is not important (CI=8.91%), is very typical for its class (CU=100.0%).', 'The feature \"mean fractal dimension\", which is not important (CI=1.54%), is not typical for its class (CU=0.1%).', 'The feature \"radius error\", which is not important (CI=9.35%), is not typical for its class (CU=0.1%).', 'The feature \"texture error\", which is not important (CI=6.53%), is very typical for its class (CU=100.0%).', 'The feature \"perimeter error\", which is not important (CI=1.48%), is not typical for its class (CU=0.1%).', 'The feature \"area error\", which is very important (CI=57.97%), is not typical for its class (CU=0.1%).', 'The feature \"smoothness error\", which is not important (CI=16.51%), is not typical for its class (CU=0.1%).', 'The feature \"compactness error\", which is not important (CI=4.39%), is not typical for its class (CU=0.1%).', 'The feature \"concavity error\", which is not important (CI=4.03%), is not typical for its class (CU=0.1%).', 'The feature \"concave points error\", which is not important (CI=5.76%), is very typical for its class (CU=100.0%).', 'The feature \"symmetry error\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"fractal dimension error\", which is not important (CI=21.47%), is not typical for its class (CU=17.33%).', 'The feature \"worst radius\", which is not important (CI=1.27%), is very typical for its class (CU=100.0%).', 'The feature \"worst texture\", which is very important (CI=60.61%), is not typical for its class (CU=13.75%).', 'The feature \"worst perimeter\", which is important (CI=41.37%), is not typical for its class (CU=23.17%).', 'The feature \"worst area\", which is not important (CI=19.51%), is typical for its class (CU=67.91%).', 'The feature \"worst smoothness\", which is not important (CI=18.24%), is unlikely for its class (CU=48.97%).', 'The feature \"worst compactness\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).', 'The feature \"worst concavity\", which is not important (CI=10.79%), is very typical for its class (CU=100.0%).', 'The feature \"worst concave points\", which is important (CI=42.94%), is not typical for its class (CU=4.32%).', 'The feature \"worst symmetry\", which is not important (CI=5.86%), is not typical for its class (CU=0.1%).', 'The feature \"worst fractal dimension\", which is not important (CI=0.0%), is not typical for its class (CU=0.1%).']\r\n\r\nDrawbacks\r\nAlthough CIU is a brilliant and simple technique, I believe it has the following drawbacks:\r\nIn regression problems, the range of possible values for the target variable can be infinite, which somehow does not make sense when we want to compute CIU. The authors said that they had put a limit on the range of values.\r\nComputing the range of values can be a little bit misleading, especially when we have outliers in the dataset.\r\nIt is not clear how we can get a global explanation for the model using CIU.\r\n",
    "preview": {},
    "last_modified": "2021-06-06T14:46:51+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-22-covid-in-nl/",
    "title": "Covid-19 Trends in the Netherlands",
    "description": "Based on data published by RIVM in this post, I looked at how Covid-19 cases spread throughout the Netherlands.",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2021-05-22",
    "categories": [
      "R",
      "Data Visualizaion",
      "Covid-19"
    ],
    "contents": "\r\nTwo weeks ago, I made a visualization that shows how Covid-19 cases spread in the Netherlands from the beginning of March and how grim the situation looks. However, someone pointed out to the fact that the number of tests has increased significantly. It means that my plot may exaggerate the Covid-19 situation in the Netherlands. Unfortunately, I could not find testing data for each Dutch municipality. Instead, I decided to use hospitalization admission and deceased cases to see if we can indeed see a massive spread in the second wave of Covid-19 cases in the Netherlands.\r\n\r\n\r\nShow code\r\nlibrary(tidyverse)\r\nlibrary(CoronaWatchNL)\r\nlibrary(sf)\r\nlibrary(gganimate)\r\nlibrary(santoku)\r\nlibrary(lubridate)\r\nlibrary(foreign)\r\ntheme_set(theme_void())\r\ntheme_update(\r\n  #plot.background = element_rect(fill = '#FDF6E3',color = '#FDF6E3'),\r\n  text = element_text(family = 'Poppins Light'),\r\n  plot.subtitle = element_text(\r\n    family = 'Poppins Light',\r\n    size = 10,\r\n    margin = margin(b = 10)\r\n  ),\r\n  plot.title = element_text(\r\n    family = 'Poppins Light',\r\n    size = 12,\r\n    margin = margin(t = 10, b = 10)\r\n  )\r\n)\r\n\r\nI created an R package called CoronaWarchNL that allows you to access a wide range of Covid-19 datasets. I’ll use this package in this post to get Covid-19 cases, hospital admissions, and deaths for Dutch municipalities.\r\n\r\n\r\nShow code\r\nmunicipalBoundaries <- st_read(\r\n    \"https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetFeature&service=WFS&version=2.0.0&typeName=cbs_gemeente_2020_gegeneraliseerd&outputFormat=json\"\r\n  )\r\n\r\n\r\ndaily_cases_per_municpality <- get_daily_cases_per_municipality()\r\npopulatuon_per_region <- get_population_per_region()\r\n\r\ndaily_cases_per_municpality <- daily_cases_per_municpality %>%\r\n  inner_join(populatuon_per_region, by = c('Municipality_name' = 'Regions')) %>%\r\n  mutate(\r\n    Date_of_publication = as_date(Date_of_publication),\r\n    avg_daily_total_cases = 100000 * as.numeric(Total_reported) / as.numeric(`Bevolking op 1 januari (aantal)`),\r\n    avg_daily_hospital_admissions = 100000 * as.numeric(Hospital_admission) / as.numeric(`Bevolking op 1 januari (aantal)`),\r\n    avg_daily_deceased = 100000 * as.numeric(Deceased) / as.numeric(`Bevolking op 1 januari (aantal)`)\r\n  )\r\n\r\nI compute the weekly average number of Covid-19 cases, hospitalizations, and death per 100000 inhabitants in each municipality in the Netherlands. The following piece of code shows how I did this using R.\r\n\r\n\r\nShow code\r\nweekly_cases <- daily_cases_per_municpality %>%\r\n  mutate(week = round_date(Date_of_publication , unit = 'week'))\r\n\r\nweekly_cases_per_municpality <- weekly_cases %>%\r\n  group_by(Municipality_name, week) %>%\r\n  summarise(\r\n    avg_weekly_total_cases = mean(avg_daily_total_cases),\r\n    avg_weekly_hospital_admissions = mean(avg_daily_hospital_admissions),\r\n    avg_weekly_deceased = mean(avg_daily_deceased)) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    d_avg_weekly_total_cases = chop(avg_weekly_total_cases, c(0, 0, 0.5, 1, 5, 12, 20, 35, 55, 80, 100)),\r\n    d_avg_hospital_admissions = chop(\r\n      avg_weekly_hospital_admissions,\r\n      c(0, 0, 0.5, 1, 2, 3, 5, 7, 9, 10, 15)),\r\n    d_avg_weekly_deceased = chop(avg_weekly_deceased, c(0, 0, 0.1, 0.5, 1, 1.5, 2, 2.5, 3, 5)))\r\n\r\ndata_weekly <- municipalBoundaries %>%\r\n  right_join(weekly_cases_per_municpality,\r\n             by = c(statnaam = \"Municipality_name\"))\r\n\r\nI will create an animation that shows how Covid-19 cases spread in the Netherlands and which municipality were and are hit hardest by the pandemic.\r\n\r\n\r\nShow code\r\nmake_animation <- function(data, var_name, pal, title) {\r\n  var_name <- rlang::enquo(var_name)\r\n  data %>%\r\n    #filter(week > '2020-10-01') %>%\r\n    ggplot() +\r\n    geom_sf(aes(fill = !!var_name), color = 'gray95') +\r\n    scale_fill_manual(values  = pal) +\r\n    coord_sf(datum = NA) +\r\n    labs(\r\n      title = title,\r\n      subtitle = 'Date: {current_frame}',\r\n      fill = 'Counts per 100000',\r\n      caption = 'Source: RIVM'\r\n    ) +\r\n    transition_manual(week, cumulative = T) +\r\n    ease_aes(\"sine\") +\r\n    enter_fade(alpha = 0.5) +\r\n    exit_fade(alpha = 0.5)\r\n}\r\n\r\nThe first animation shows the number of infections in each municipality, from the start of the pandemic in February until recently. As you can see, the second wave, which began in late September, looks really terrifying. Note that there are some municipalities that no data is available for them.\r\n\r\n\r\nShow code\r\npal_cases <- c(\r\n      'gray95',\r\n      '#fee440',\r\n      '#FFBA08',\r\n      '#FAA307',\r\n      '#F48C06',\r\n      '#E85D04',\r\n      '#DC2F02',\r\n      '#D00000',\r\n      '#9D0208',\r\n      '#6A040F',\r\n      '#370617',\r\n      '#03071e'\r\n    )\r\nmake_animation(data_weekly,d_avg_weekly_total_cases,pal_cases,'The Average Weekly Number of Covid-19 Cases\\nper 100000 Inhabitants in the Netherlands')\r\n\r\n\r\nIf we look at the number of hospital admissions we see a different story. It seems that the number of hospitalizations was higher during the first wave of Covid-19 compared to the second wave and mostly the southern parts of the Netherlands were hit harder than the rest of the Netherlands.\r\n\r\n\r\nShow code\r\npal_patients <- c(\r\n      'gray95',\r\n      '#caf0f8',\r\n      '#ade8f4',\r\n      '#90e0ef',\r\n      '#48cae4',\r\n      '#00b4d8',\r\n      '#0096c7',\r\n      '#0077b6',\r\n      '#023e8a',\r\n      '#03045e',\r\n      '#03071e'\r\n)\r\nmake_animation(data_weekly,d_avg_hospital_admissions,pal_patients,'The Average Weekly Number of Hospital Admissions\\nper 100000 Inhabitants in the Netherlands')\r\n\r\n\r\nThe trend for deceased patients looks similar to of the hospital admission. The average numb of death during the first wave of Corona was higher than the average number of death during the second wave.\r\n\r\n\r\nShow code\r\npal_deceased <- c(\r\n      'gray95',\r\n      '#fdc5f5',\r\n     '#e0aaff',\r\n      '#c77dff',\r\n      '#9d4edd',\r\n      '#7b2cbf',\r\n      '#5a189a',\r\n      '#3c096c',\r\n      '#240046',\r\n      '#10002b'\r\n\r\n)\r\nmake_animation(data_weekly,d_avg_weekly_deceased,pal_deceased,'The Average Weekly Number of Deceased Patients\\nper 100000 Inhabitants in the Netherlands')\r\n\r\n\r\nThese different animations show us two distinct trends. On the one hand, we can see the number of confirmed cases rose rapidly during the second wave to affect almost all regions. On the other hand, the number of hospitalizations and deaths during the second wave slightly decreased. This might suggest that the rise in the number of cases is mainly driven by an increase in the number of tests. Alternatively, the virus might have become less deadly and severe.\r\n\r\n\r\n",
    "preview": "posts/2021-05-22-covid-in-nl/featured.JPG",
    "last_modified": "2021-06-06T14:24:49+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-22-how-easy-is-it-to-understand-what-donald-trump-says/",
    "title": "How Easy Is It to Understand What Donald Trump Says?",
    "description": "The computational complexity of the language that Trump uses in his speeches",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2020-11-03",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nFlesch Reading Ease scores\r\nGunning fog index\r\nThe SMOG index\r\nLinsear Write Formula\r\nDale-Chall Readability Score\r\nA unified readability\r\nConclusion\r\n\r\n\r\nAside from their political differences, Donald Trump and Barack Obama have very contrasting personalities, traits and characters. Obama is known to be a great communicator and an articulate speaker whose speeches are used in English classes to show how one should speak proper English. On the other hand, Trump is not the most eloquent English speaker or US president in history. Every now and then, you can find a clip on the web where Donald Trump is being mocked for the way he speaks or mispronounces words. This is so obvious that even non-native English speakers can notice how Trump’s speeches are very simple and inarticulate. Of course, this was not a bad thing for Trump at all. Actually, almost every political analyst that you see on the news talks about the fact that a vast majority of Trump’s fervent supporters are not college-educated Americans. We can attribute this to the fact that he knows how to speak to his audience and his base supporters using their language (Although it is more likely that he cannot speak better English better than this level).\r\nThis post will investigate how difficult it is to understand what each US politicians talked about in the 2020 US Election cycle. I will use several readability metrics that can help us compute text comprehensibility. A wide range of these measures are implemented in the {textstat} python package, and it is super easy to calculate them using this package.\r\nI compiled a list of US Election-related speeches from rev.com and turned them into an R package called {us2020election}. I use this package as my data source for my analysis. Like some of my other posts, I use Python to perform the analysis and R to visualize my results. Now let’s get started by importing the necessary packages.\r\n\r\n\r\nShow code\r\nlibrary(tidyverse)\r\nlibrary(reticulate)\r\nlibrary(ggthemes)\r\nlibrary(us2020election)\r\nlibrary(ggridges)\r\ntheme_set(theme_tufte())\r\ntheme_update(legend.position = 'none',\r\n          text = element_text(family = 'Lobser'),\r\n          plot.title = element_text(margin = margin(t= 10,b= 5),family = 'Lobser'),\r\n          plot.subtitle = element_text(margin = margin(b= 10),family = 'Lobser'),\r\n          panel.background = element_rect(fill = '#FDF6E3'),\r\n          plot.background = element_rect(fill = '#FDF6E3'))\r\n\r\n\r\n\r\nShow code\r\nimport numpy as np\r\nimport pandas as pd \r\nimport textstat\r\n\r\nThere are several readability measures for English text included in {textstat}. Calculating these measures is very straightforward and easy. I will explain what each metric represents in more details.\r\n\r\n\r\nShow code\r\nus_election_speeches = r.us_election_speeches\r\nus_election_speeches['Flesch_Reading_Ease_formula'] = us_election_speeches['text'].apply(lambda x: textstat.flesch_reading_ease(x))\r\nus_election_speeches['gunning_fog'] = us_election_speeches['text'].apply(lambda x: textstat.gunning_fog(x))\r\nus_election_speeches['smog_index'] = us_election_speeches['text'].apply(lambda x: textstat.smog_index(x))\r\nus_election_speeches['automated_readability_index'] = us_election_speeches['text'].apply(lambda x: textstat.automated_readability_index(x))\r\nus_election_speeches['coleman_liau_index'] = us_election_speeches['text'].apply(lambda x: textstat.coleman_liau_index(x))\r\nus_election_speeches['linsear_write_formula'] = us_election_speeches['text'].apply(lambda x: textstat.linsear_write_formula(x))\r\nus_election_speeches['dale_chall_readability_score'] = us_election_speeches['text'].apply(lambda x: textstat.dale_chall_readability_score(x))\r\nus_election_speeches['text_standard'] = us_election_speeches['text'].apply(lambda x: textstat.text_standard(x))\r\nus_election_speeches['text_standard_float'] = us_election_speeches['text'].apply(lambda x: textstat.text_standard(x,float_output  = True))\r\n\r\nLet’s look at the resulting dataframe.\r\n\r\n\r\nShow code\r\nus_election_speeches <- py$us_election_speeches \r\nus_election_speeches %>% \r\nglimpse()\r\nRows: 286\r\nColumns: 15\r\n$ speaker                      <chr> \"Barack Obama\", \"Mike Pence\", \"~\r\n$ title                        <chr> \"Barack Obama Campaign Roundtab~\r\n$ text                         <chr> \"Barack Obama: (00:01)\\n… or th~\r\n$ date                         <chr> \"Oct 21, 2020\", \"Oct 21, 2020\",~\r\n$ location                     <chr> \"Philadelphia, Pennsylvania\", \"~\r\n$ type                         <chr> \"Roundtable\", \"Campaign Speech\"~\r\n$ Flesch_Reading_Ease_formula  <dbl> 78.38, 67.99, 65.35, 85.99, 71.~\r\n$ gunning_fog                  <dbl> 8.80, 9.32, 10.80, 5.37, 8.30, ~\r\n$ smog_index                   <dbl> 9.8, 11.5, 11.6, 8.1, 10.4, 8.7~\r\n$ automated_readability_index  <dbl> 9.0, 10.8, 11.5, 5.3, 8.8, 6.9,~\r\n$ coleman_liau_index           <dbl> 7.95, 9.11, 8.71, 6.48, 8.12, 6~\r\n$ linsear_write_formula        <dbl> 5.375000, 5.333333, 11.666667, ~\r\n$ dale_chall_readability_score <dbl> 5.77, 5.75, 6.27, 5.18, 5.65, 5~\r\n$ text_standard                <chr> \"8th and 9th grade\", \"8th and 9~\r\n$ text_standard_float          <dbl> 9, 9, 12, 6, 8, 6, 6, 5, 11, 6,~\r\n\r\nNow I am going to visualize the changes in the distribution of speech complexity for each politician. To make things more, I will select a list of politicians that I’d like to analyze in this post.\r\n\r\n\r\nShow code\r\nspeakers <- c('Barack Obama','Pete Buttigieg','Mike Pence','Elizabeth Warren','Bernie Sanders','Donald Trump','Kamala Harris','Joe Biden','Mike Bloomberg')\r\ncustom_palette <-c(\r\n    'Mike Bloomberg' = '#4E79A7',\r\n    'Amy Klobuchar' = '#4E79A7',\r\n    'Joe Biden' = '#4E79A7',\r\n    'Pete Buttigieg' = '#4E79A7',\r\n    'Elizabeth Warren' =  '#4E79A7',\r\n    'Barack Obama'  = '#4E79A7',\r\n    'Bernie Sanders' = '#4E79A7',\r\n    'Kamala Harris' = '#4E79A7',\r\n    'Donald Trump'  = '#E15759' ,\r\n     'Mike Pence' = '#E15759' \r\n  )\r\n\r\nAlso, I created a function to make ridge plots for each metric easier.\r\n\r\n\r\nShow code\r\ncreate_plot <- function(metric = Flesch_Reading_Ease_formula,subtitle = subtitle) {\r\n  metrics <- rlang::enquo(metric)\r\n  us_election_speeches %>%\r\n    separate_rows(speaker, sep = ',') %>%\r\n    filter(speaker %in% speakers, type != 'Debate') %>%\r\n    add_count(speaker) %>%\r\n    ggplot() +\r\n    geom_density_ridges(aes(\r\n      x = !!metrics ,\r\n      y = speaker,\r\n      fill = speaker\r\n    )) +\r\n    labs(x = '', y = '',title = \"How Easy Is It to Comprehend Different US Politicians?\",subtitle = str_wrap(subtitle,width = 100)) +\r\n    scale_fill_manual(values = custom_palette) \r\n}\r\n\r\nNow, let’s look at several readability measure in more depth.\r\nFlesch Reading Ease scores\r\nThe first readability score that I will look at is based on the Flesch Reading Ease formula. It computes the number of syllables to determine how easy a piece of text is. The maximum value of Flesch Reading Ease is 122, and there is no minimum value for it. Higher Flesch Reading Ease scores indicate that the text (speech) is easier to understand by the audience. In our case, it would show how sophisticated each politician is in terms of language use. You can find more about this metric on Wikipedia!\r\n\r\n\r\nShow code\r\ncreate_plot(Flesch_Reading_Ease_formula ,\r\n            subtitle = 'The Flesch Reading Ease scores measure the complexity of a text document. Higher scores indicate a text is easier to comprehend.')\r\n\r\n\r\nwe can interpret the scores using the following table:\r\nScore\r\nSchool level\r\nNotes\r\n100.00–90.00\r\n5th grade\r\nVery easy to read. Easily understood by an average 11-year-old student.\r\n90.0–80.0\r\n6th grade\r\nEasy to read. Conversational English for consumers.\r\n80.0–70.0\r\n7th grade\r\nFairly easy to read.\r\n70.0–60.0\r\n8th & 9th grade\r\nPlain English. Easily understood by 13- to 15-year-old students.\r\n60.0–50.0\r\n10th to 12th grade\r\nFairly difficult to read.\r\n50.0–30.0\r\nCollege\r\nDifficult to read.\r\n30.0–10.0\r\nCollege graduate\r\nVery difficult to read. Best understood by university graduates.\r\n10.0–0.0\r\nProfessional\r\nExtremely difficult to read. Best understood by university graduates.\r\nGunning fog index\r\nThe Gunning fog index is another metric to measure the complexity of a text document. It shows how many years of education one might need to understand a piece of text. Larger values of the Gunning fog index correspond to more difficult writings.\r\n\r\n\r\nShow code\r\ncreate_plot(gunning_fog,subtitle = 'The Gunning fog index measure the complexity of a text document. Larger values of the Gunning fog index correspond to more difficult writings.' )\r\n\r\n\r\nThe SMOG index\r\nThe SMOG index computes the ratio of polysyllables (words with three or more syllables) in sentences to determine text complexity.\r\n\r\n\r\nShow code\r\ncreate_plot(smog_index,subtitle = 'The SMOG index measure the complexity of a text document. Larger values of the SMOG index indicate more difficult writings.' )\r\n\r\n\r\nLinsear Write Formula\r\nLike previous the metric, the Linsear Write Formula uses words with three or more syllables to compute text readability. It also relies on the sentence length to measure how difficult reading a text could be.\r\n\r\n\r\nShow code\r\ncreate_plot(linsear_write_formula, subtitle = 'The Linsear Write Formula measure the complexity of a text document. Larger values indicate more difficult writings.')\r\n\r\n\r\nDale-Chall Readability Score\r\nThis metric is different from the other metrics that we have talked about. It uses a dictionary of 3000 words that are easy to read and understand for a fourth-grade student. So, Words that are not in this dictionary are considered to be complex. The higher the Dale-Chall Score is, the more difficult it is to read a text.\r\n\r\n\r\nShow code\r\ncreate_plot(dale_chall_readability_score,subtitle = 'The Dale-Chall Readability Score measure the complexity of a text document. The higher the Dale-Chall Score is, the more difficult it is to read a text.')\r\n\r\n\r\nA unified readability\r\nWe introduced several readability metrics, but each one of them might give us a slightly different result. There is a way in textstats to combine all these metrics and have a single readability metric.\r\n\r\n\r\nShow code\r\nus_election_speeches %>%\r\n  filter(speaker %in% speakers) %>%\r\n  mutate(text_standard = str_replace(text_standard,' and ','-'),\r\n        text_standard = factor(\r\n    text_standard,\r\n    levels = c(\r\n      '4th-5th grade',\r\n      '5th-6th grade',\r\n      '6th-7th grade',\r\n      '7th-8th grade',\r\n      '8th-9th grade',\r\n      '9th-10th grade',\r\n      '10th-11th grade',\r\n      '11th-12th grade',\r\n      '12th-13th grade',\r\n      '14th-15th grade'\r\n    )\r\n  )) %>%\r\n  count(speaker, text_standard) %>%\r\n  mutate(n = n + 1) %>%\r\n  ggplot()  +\r\n  geom_col(aes(x = text_standard , y =  n, fill = speaker)) +\r\n  labs(x = '', y = 'Number of Speeches', title = \"How Easy Is It to Understand 'Trump's Speeches?\",\r\n       subtitle = 'Based on several readability tests, the education level that one needs to comprehend the 2020 Election speeches by different US politicians is illustrated in this plot.' ) +\r\n  scale_fill_manual(values = custom_palette) +\r\n  scale_y_log10() +\r\n  facet_wrap(~ speaker, ncol = 1) +\r\n  theme(axis.text  = element_text(size = 13),\r\n        axis.title.y = element_text(size = 15,margin = margin(r = 10,l = 10)),\r\n        plot.title = element_text(size = 20,margin = margin(b = 10,t = 10)),\r\n        plot.subtitle = element_text(size = 14,margin = margin(b = 10)),\r\n        strip.text = element_text(size = 15))\r\n\r\n\r\nInterestingly, we can observe that Trump never gave a speech to an audience with difficulty more than the 7th or 8th grade. We can also convert this readability metric to numbers to visualize and compare it to other metrics.\r\n\r\n\r\nShow code\r\ncreate_plot(text_standard_float,\r\n            subtitle = 'The complexity of a text document were measured based on several readability metrics where larger values indicate more difficult writings.')\r\n\r\n\r\nConclusion\r\nWe can consistently see that Trump speeches are less sophisticated and less complex than the speeches given by the rest of politicians. We can attribute this to his lack of sophistication in terms of language, the fact that he knows how can speak to his audience or both. Also, we can notice that Mike Pence and Barack Obama seem to use a more an advanced language in their speeches.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-22-how-easy-is-it-to-understand-what-donald-trump-says/featured.JPG",
    "last_modified": "2021-06-06T14:49:20+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-15-which-presidential-debate-was-more-chaotic/",
    "title": "Which Presidential Debate Was More Chaotic?",
    "description": "In this post, I looked at how Biden and Trump interrupted each other during the first Presidential debate and compared it with the recent Presidential debates.",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2020-10-15",
    "categories": [
      "R",
      "NLP"
    ],
    "contents": "\r\n\r\nContents\r\nBiden and Trump’s first debate\r\nClinton vs. Trump\r\nVice Presidential debates\r\n\r\nMany people watched the first Presidential Debate between Biden and Trump and thought that this debate was chaotic, full of vulgar language, interruptions, and in a word, really ugly! Some people even consider this debate as the worst debate in the modern history of US Presidential Elections! Four years ago, Trump was also a presidential candidate and ran against Hillary Clinton. The Presidential Debates in 2016 were not exceptionally friendly or civilized. So, the question is what made the 2020 first debate unique and chaotic in many people’s minds. In this blog post, I will investigate this question and compare the 2020 and 2016 Presidential debates and the Vice Presidential debates.\r\n\r\n\r\nShow code\r\nlibrary(tidyverse) # metapackage of all tidyverse packages\r\nlibrary(lubridate)\r\nlibrary(readxl)\r\nlibrary(ggthemes)\r\nlibrary(showtext)\r\nlibrary(plotly)\r\nfont_add_google(\"Lobster\", \"Lobster\")\r\nfont_add_google(\"Overpass\", \"Overpass\")\r\noptions(repr.plot.width=20, repr.plot.height=15)\r\n\r\nbiden_col <- '#118ab2'\r\ntrump_col <- '#ef476f'\r\nwallace_col <- '#ffd166'\r\ncross_talk_col <- '#06d6a0'\r\nmoderator <- '#ffd166'\r\naudience <- '#e36414'\r\nclinton_col <- '#118ab2'\r\ntext_col <-  'gray80'\r\n\r\ntheme_set(theme_void())\r\ntheme_update(  \r\n    legend.position = 'top',\r\n    legend.text = element_text(\r\n      size = 20,\r\n      family = 'Lobster',\r\n      color = text_col\r\n    ),\r\n    text = element_text(family = 'Lobster', color = text_col),\r\n    plot.title = element_text(\r\n      size = 40,\r\n      margin = margin(b = 40, t = 50, l = 50),\r\n      hjust = 0.5,\r\n      family = 'Lobster',\r\n      color = text_col\r\n    ),\r\n    plot.caption = element_text(\r\n      size = 20,\r\n      ,\r\n      margin = margin(b = 50, t = 50),\r\n      family = 'Lobster',\r\n      color = text_col\r\n    ),\r\n    plot.background = element_rect(fill = 'gray14'),\r\n    panel.background = element_rect(fill = 'gray14')\r\n  )\r\n\r\nBiden and Trump’s first debate\r\nLet us look at the first debate between Trump and Biden and how much each candidate used to speak uninterrupted by the other candidate or the moderator. I used this dataset which is available on Kaggle to computed how many seconds Trump and Biden talked without being cut off by the other candidate.\r\n\r\n\r\nShow code\r\npresidential_debate_2020 <- read_delim('presidential_debate_2020.csv',\r\n                        delim = '\\t',\r\n                        col_types = list(col_character(),\r\n                                         col_character(),\r\n                                         col_character(),\r\n                                         col_character(),\r\n                                         col_integer(),\r\n                                         col_integer(),\r\n                                         col_double(),\r\n                                         col_integer()))\r\npresidential_debate_2020 <- presidential_debate_2020 %>% \r\n  mutate(minutes = if_else(nchar(time)<6,paste('00:',time,sep = ''),time),\r\n         minutes = paste('2020-09-29',minutes,sep = ''),\r\n         minutes = lubridate::ymd_hms(minutes),\r\n         speaker = if_else(str_detect(speaker , 'Chris Wallace'),'Chris Wallace (Moderator)',speaker),\r\n          ) %>% \r\n    mutate(minute_start = lag(minutes,n= 1),\r\n         duration =minutes - minute_start,\r\n         duration = lead(duration,n =1),\r\n         seconds_in_end = lead(seconds_in),\r\n         text = str_wrap(text,width =30))\r\n\r\n\r\npresidential_debate_2020[1,5] <- 0\r\n\r\nglimpse(presidential_debate_2020)\r\nRows: 788\r\nColumns: 12\r\n$ speaker        <chr> \"Chris Wallace (Moderator)\", \"Chris Wallace (~\r\n$ text           <chr> \"Good evening from the Health\\nEducation Camp~\r\n$ url            <chr> \"https://www.rev.com/transcript-editor/shared~\r\n$ time           <chr> \"1:20\", \"2:10\", \"2:49\", \"2:51\", \"2:51\", \"3:11~\r\n$ seconds_in     <int> 0, 130, 169, 171, 171, 191, 241, 293, 322, 32~\r\n$ seconds_spoken <int> 50, 39, 2, 0, 20, 50, 52, 29, 7, 5, 2, 36, 56~\r\n$ words_per_min  <dbl> 148.8000, 156.9231, 120.0000, NA, NA, 159.600~\r\n$ num_words      <int> 124, 102, 4, 4, 2, 133, 156, 98, 15, 16, 3, 1~\r\n$ minutes        <dttm> 2020-09-29 00:01:20, 2020-09-29 00:02:10, 20~\r\n$ minute_start   <dttm> NA, 2020-09-29 00:01:20, 2020-09-29 00:02:10~\r\n$ duration       <drtn> 50 secs, 39 secs, 2 secs, 0 secs, 20 secs, 5~\r\n$ seconds_in_end <int> 130, 169, 171, 171, 191, 241, 293, 322, 329, ~\r\n\r\n\r\n\r\nShow code\r\npresidential_debate_2020 %>%\r\n  ggplot(aes(\r\n    x = seconds_in,\r\n    y = 1,\r\n    xend = seconds_in_end,\r\n    yend = 1,\r\n    color = speaker\r\n  )) +\r\n  geom_segment(size = 40, alpha = 0.7) +\r\n\r\n  guides(color = guide_legend(override.aes = list(size = 25))) +\r\n  scale_color_manual(values = c(wallace_col, trump_col, biden_col)) +\r\n  scale_y_continuous(limits = c(0.85, 1.13)) +\r\n  labs(\r\n    x = '',\r\n    y = '',\r\n    title = 'How Did The First US Presidential Debate Go?',\r\n    #subtitle = 'This plot illustrates how much time each presidential candidate spoke ',\r\n    fill = '',\r\n    color = ''  ) \r\n\r\n\r\nClinton vs. Trump\r\nThis plot clearly shows that there were many interruptions during the first debate! It also shows that it was Trump who interrupted most in the debate. To give it more context, let us compare it to the 2016 Debates between Trump and Clinton. The transcripts of these debates are available in this dataset that I found on Kaggle. However, this dataset does not include information about how many seconds or minutes each candidate spent speaking in the debates. So, I decided to use the number of words each candidate spoke to measure continuity in their speeches.\r\n\r\n\r\nShow code\r\ndebate_2016 <- read_csv('presidential_debate_2016.csv')\r\ndebate_2016 <-debate_2016 %>% \r\n    mutate(Text = as.character(Text),\r\n    num_chars = str_length(Text),\r\n    num_word = str_count(Text),\r\n    Speaker = case_when(Speaker == 'CANDIDATES'~'Crosstalk',\r\n                        Speaker == 'QUESTION'~'Question',\r\n                        Speaker %in% c('Cooper', 'Holt','Wallace','Raddatz','Quijano') ~'Moderator',\r\n                        TRUE ~ Speaker))\r\n\r\npresidential_debate_2016 <- debate_2016 %>% \r\n  filter(!Speaker %in% c('Kaine','Pence'),\r\n         Date != '10/4/16')\r\n\r\n\r\nfirst_debate <- presidential_debate_2016%>% \r\n  filter(Date == '9/26/16')\r\nsecond_debate <- presidential_debate_2016 %>% \r\n  filter(Date == '10/9/16')\r\nthird_debate <- presidential_debate_2016 %>% \r\n  filter(Date == '10/19/2016')\r\n\r\nglimpse(presidential_debate_2016)\r\nRows: 1,028\r\nColumns: 6\r\n$ Line      <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,~\r\n$ Speaker   <chr> \"Moderator\", \"Audience\", \"Clinton\", \"Audience\", \"M~\r\n$ Text      <chr> \"Good evening from Hofstra University in Hempstead~\r\n$ Date      <chr> \"9/26/16\", \"9/26/16\", \"9/26/16\", \"9/26/16\", \"9/26/~\r\n$ num_chars <int> 1257, 10, 20, 10, 17, 10, 1115, 820, 1018, 171, 15~\r\n$ num_word  <int> 1257, 10, 20, 10, 17, 10, 1115, 820, 1018, 171, 15~\r\n\r\n\r\n\r\nShow code\r\nfirst_debate<- first_debate %>% \r\n  mutate(cumsum_nwords = cumsum(num_word),\r\n         cum_sum_lag = lag(cumsum_nwords),\r\n         debate = 'First Debate')\r\n\r\nfirst_debate[1,7] <- 0\r\n\r\nsecond_debate<- second_debate %>% \r\n  mutate(cumsum_nwords = cumsum(num_word),\r\n         cum_sum_lag = lag(cumsum_nwords),\r\n         debate = 'Second Debate')\r\n\r\nsecond_debate[1,7] <- 0\r\n\r\nthird_debate<- third_debate %>% \r\n  mutate(cumsum_nwords = cumsum(num_word),\r\n         cum_sum_lag = lag(cumsum_nwords),\r\n         debate = 'Third Debate')\r\n\r\nthird_debate[1,7] <- 0\r\n\r\n\r\npresidential_debates_2016 <- bind_rows(first_debate,second_debate ,third_debate)\r\n\r\n\r\n\r\nShow code\r\npresidential_debates_2016 %>%\r\nggplot(aes(\r\n    x = cum_sum_lag,\r\n    y = 1,\r\n    xend = cumsum_nwords,\r\n    yend = 1,\r\n    color = Speaker\r\n  )) +\r\n  geom_segment(size = 40, alpha = 0.7) +\r\n\r\n  guides(color = guide_legend(override.aes = list(size = 18))) +\r\n  scale_color_manual(values = c('Moderator' = moderator ,'Trump' = trump_col  ,\r\n                               'Clinton' = clinton_col ,\r\n                              'Crosstalk'  ='#9d4edd',\r\n                              'Audience' =  audience ,\r\n                              'Question'  =  '#e85d04')) +\r\n  scale_y_continuous(limits = c(0.85, 1.13)) +\r\n  labs(\r\n    x = '',\r\n    y = '',\r\n    title = 'How Did 2016 US Presidential Debates Go?',\r\n    fill = '',\r\n    color = '',\r\n    caption =  \r\n  ) +\r\n  facet_wrap(~debate,nrow = 3) +\r\n  theme(strip.text = element_text(size = 15))\r\n\r\n\r\nWell, the debates between Trump and Clinton were also wild, but they were less anarchic than the debate between Biden and Trump. For, there were fewer disruptions in those debates compared to what we saw in the 2020 plot.\r\nVice Presidential debates\r\nNow let’s look at the debates between the Vice Presidential candidates. Usually, these debates are more civilized and less heated as they attract less attention.\r\nHarris vs. Pence\r\nI found a dataset of the 2020 VP debate on Kaggle. Again, here I used the same approach that I used for the 2016 debate.\r\n\r\n\r\nShow code\r\nvice_presidential_debate_2020 <- read_csv('vice_presidential_debate_2020.csv')\r\nvice_presidential_debate_2020 <- vice_presidential_debate_2020 %>%\r\nmutate(num_chars = str_length(text ),\r\n       cumsum_nwords = cumsum(num_chars),\r\n      cum_sum_lag = lag(cumsum_nwords))\r\n\r\nvice_presidential_debate_2020[1,6] <- 0\r\n\r\n\r\n\r\nShow code\r\nvice_presidential_debate_2020 %>%\r\nggplot(aes(\r\n    x = cum_sum_lag,\r\n    y = 1,\r\n    xend = cumsum_nwords,\r\n    yend = 1,\r\n    color = speaker \r\n  )) +\r\n  geom_segment(size = 40, alpha = 0.7) +\r\n\r\n  guides(color = guide_legend(override.aes = list(size = 18))) +\r\n  scale_color_manual(values = c(\"Susan Page\" = moderator ,'Mike Pence' = trump_col  ,\r\n                               'Kamala Harris' = clinton_col )) +\r\n  scale_y_continuous(limits = c(0.85, 1.13)) +\r\n  labs(\r\n    x = '',\r\n    y = '',\r\n    title = 'How Did 2020 US Vice-Presidential Debates Go?',\r\n    fill = '',\r\n    color = '',\r\n    caption =  \r\n  )\r\n\r\n\r\n2016 Vice Presidential debate\r\n\r\n\r\nShow code\r\nvp_debates <- debate_2016 %>% \r\n  filter(Date == '10/4/16') %>% \r\n  mutate(cumsum_nwords = cumsum(num_word),\r\n         cum_sum_lag = lag(cumsum_nwords)) \r\n  \r\nvp_debates[1,5] <- 0\r\n\r\n\r\n\r\nShow code\r\nvp_debates %>%\r\nggplot(aes(\r\n    x = cum_sum_lag,\r\n    y = 1,\r\n    xend = cumsum_nwords,\r\n    yend = 1,\r\n    color = Speaker \r\n  )) +\r\n  geom_segment(size = 40, alpha = 0.7) +\r\n\r\n  guides(color = guide_legend(override.aes = list(size = 18))) +\r\n  scale_color_manual(values = c('Moderator' = moderator ,'Pence' = trump_col  ,\r\n                               'Kaine' = clinton_col ,\r\n                              'Crosstalk'  ='#9d4edd',\r\n                              'Audience' =  audience )) +\r\n  scale_y_continuous(limits = c(0.85, 1.13)) +\r\n  labs(\r\n    x = '',\r\n    y = '',\r\n    title = 'How Did 2016 US Vice-Presidential Debates Go?',\r\n    fill = '',\r\n    color = '',\r\n    caption =  \r\n  )\r\n\r\n\r\nWell, we can see that Pence and Kaine cut each other off many times during the VP debate. However, we can also observe that they could speak uninterrupted on some occasions.\r\n\r\n\r\n",
    "preview": "posts/2020-10-15-which-presidential-debate-was-more-chaotic/featured.JPG",
    "last_modified": "2021-06-06T14:51:11+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-04-persiansongs/",
    "title": "The Happiest, Saddest, Most Energetic and Most Popular Persian Singers on Spotify",
    "description": "I investigate the difference between audio features of Iranian songs and singers on Spotify.",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2020-05-04",
    "categories": [
      "R",
      "Visualization"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData Collection\r\nOverall Song Features\r\nLooking more closely at each audio feature\r\nHappiness\r\nEnergy\r\nAcousticness\r\nDanceability\r\nLoudness\r\n\r\nMost Popular Songs\r\n\r\nIntroduction\r\nI am a music lover, and like my other hobbies, I am really interested in applying data science methods to it. A few months ago, I participated in the third week of the TidyTuesday project, where I made a map of Spotify songs based on audio features and a dimensionality reduction algorithm called UMAP. Since then, I have been using Spotify’s Web API to collect data, and recently, I decided to look at some of my favorite Iranian artists and their songs on Spotify. We have different genres and types of music, and while pop and rap are very popular among the younger generation, I like the traditional style more. Nevertheless, I was always curious to understand how different traditional music and pop music are. For this reason, that I like the most These are a few questions that I would like to answer:\r\nHow different audio features can be among top Persian singers?\r\nWhat are the most danceable and least danceable Persian songs?\r\nWho is the most popular Persian singer, and what is the most popular song?\r\n\r\n\r\nShow code\r\nlibrary(kableExtra)\r\nlibrary(tidyverse)\r\nlibrary(googlesheets4)\r\nlibrary(tidymodels)\r\nlibrary(gghighlight)\r\nlibrary(hrbrthemes)\r\nlibrary(ggthemes)\r\nlibrary(ggrepel)\r\nlibrary(ggalt)\r\nlibrary(extrafont)\r\nlibrary(ggtext)\r\n\r\nData Collection\r\nI compiled a list of Persian Singers manually and collected information about their available songs on Spotify using the spotifyr package in R which lets us use R to access the Spotify’s API. This process was cumbersome as sometimes I was not getting what I was looking for. For instance, sometimes, songs that belonged to another random artist were retrieved. For each singer, we can only retrieve the top 10 popular songs. It means that the rest of the songs have no popularity scores. In the end, I collected various kinds of information about more than 10000 songs.\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop <- read_csv('https://raw.githubusercontent.com/mcnakhaee/datasets/master/Persian_Songs_Spotify.csv',\r\n                                  )\r\n\r\nglimpse(songs_audio_plus_pop) \r\nRows: 10,632\r\nColumns: 32\r\n$ track_id           <chr> \"31iPeC6I0AiRW8InOxNKzm\", \"4Fi46ha8teWYTw~\r\n$ poet               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r\n$ disc_number        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\r\n$ duration_ms        <dbl> 446880, 851920, 293160, 648720, 273480, 2~\r\n$ explicit           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,~\r\n$ track_name         <chr> \"Ghazale Taze\", \"Ayeeneye Hosn\", \"Tarke E~\r\n$ artist_name        <chr> \"Salar Aghili\", \"Salar Aghili\", \"Salar Ag~\r\n$ popularity         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r\n$ track_number       <dbl> 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9,~\r\n$ album_href         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r\n$ album_id           <chr> \"6GcmAWrnnMb2BuVriPhBLa\", \"6GcmAWrnnMb2Bu~\r\n$ album_name         <chr> \"Va Eshgh Amad\", \"Va Eshgh Amad\", \"Va Esh~\r\n$ album_release_date <chr> \"2/3/2020\", \"2/3/2020\", \"2/3/2020\", \"2/3/~\r\n$ album_total_tracks <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r\n$ album_release_year <dbl> 2020, 2020, 2020, 2020, 2020, 2019, 2019,~\r\n$ track_href         <chr> \"https://api.spotify.com/v1/tracks/31iPeC~\r\n$ danceability       <dbl> 0.437, 0.379, 0.437, 0.488, 0.301, 0.577,~\r\n$ energy             <dbl> 0.390, 0.146, 0.453, 0.138, 0.443, 0.366,~\r\n$ key                <dbl> 0, 5, 5, 2, 0, 0, 6, 9, 9, 2, 2, 5, 9, 7,~\r\n$ loudness           <dbl> -7.170, -10.008, -5.392, -12.287, -5.702,~\r\n$ mode               <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\r\n$ speechiness        <dbl> 0.0299, 0.0414, 0.0349, 0.0451, 0.0334, 0~\r\n$ acousticness       <dbl> 0.839, 0.970, 0.664, 0.915, 0.657, 0.834,~\r\n$ instrumentalness   <dbl> 3.51e-05, 3.60e-04, 2.07e-03, 6.58e-03, 8~\r\n$ liveness           <dbl> 0.1360, 0.0812, 0.1100, 0.2120, 0.1200, 0~\r\n$ valence            <dbl> 0.3300, 0.3460, 0.5010, 0.4450, 0.4100, 0~\r\n$ tempo              <dbl> 131.913, 105.634, 94.651, 110.967, 148.05~\r\n$ time_signature     <dbl> 3, 4, 5, 5, 1, 3, 4, 3, 4, 5, 1, 4, 4, 3,~\r\n$ key_name           <chr> \"C\", \"F\", \"F\", \"D\", \"C\", \"C\", \"F#\", \"A\", ~\r\n$ mode_name          <chr> \"minor\", \"major\", \"minor\", \"minor\", \"mino~\r\n$ key_mode           <chr> \"C minor\", \"F major\", \"F minor\", \"D minor~\r\n$ artist_id          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r\n\r\nOverall Song Features\r\nApart from variables such as the album that a song belongs to and its date of release, Spotify’s API can give us several features that capture a song’s different audio characteristics.\r\nYou can see a full list of these features in this link. However, I am only interested in some of these features, such as:\r\nvalence measures the happiness of a song.\r\nenergy is relatively self-explanatory.\r\ntempo measures the speed of a song.\r\nloudness is also self-explanatory.\r\nacousticness identifies whether the track is acoustic\r\ninstrumentalness shows whether a track contains no vocals.\r\ndanceability determines how good a song is for dancing.\r\nThis excellent visualization inspired me to create a similar plot for some of the most well-known Persian singers and see how their audio features differ from each other.\r\n\r\n\r\nShow code\r\nartists <-\r\n  c( 'Sirvan Khosravi','Hesameddin Seraj','Rastak','Shahram Nazeri','Hossein Alizadeh','Reza Sadeghi','Alireza Eftekhari','Mohammadreza Shajarian',\r\n     'Salar Aghili','Morteza Pashaei', 'Alireza Ghorbani','Homayoun Shajarian', 'Mohsen Yeganeh' ,'Morteza Pashaei','Moein','Farzad Farzin',\r\n     'Babak Jahanbakhsh', 'Ehsan Khajeh Amiri','Siavash Ghomayshi','Xaniar Khosravi','Tohi' ,'Mohsen Chavoshi','Amir Tataloo',\r\n     'Hamed Homayoun','Kayhan Kalhor')\r\n\r\nI will plot the average, the minimum, and the maximum value of each feature for each singer. That gives us a good picture of how different their audio characteristics are from each other. However, we must make the right adjustments to the dataset before visualizing it:\r\nWe need to transform the original dataset into a long-dataframe, which can be done by pivot_longer from thedplyr package.\r\nWe should rescale each audio feature, otherwise, the plot would not make any sense.\r\n\r\n\r\nShow code\r\norder <- c(\r\n  \"valence\",\r\n  \"energy\",\r\n  \"tempo\",\r\n  \"loudness\",\r\n  \"acousticness\",\r\n  \"instrumentalness\",\r\n  \"danceability\"\r\n)\r\n\r\nscaled_features_long <- songs_audio_plus_pop %>%\r\n  mutate_at(order, scales::rescale, to = c(0, 7)) %>%\r\n  filter(!is.na(popularity)) %>%\r\n  filter(artist_name %in% artists) %>%\r\n  mutate(artist_name = factor(artist_name))  %>%\r\n  pivot_longer(\r\n    names_to = 'metric',\r\n    cols = c(\r\n      \"valence\",\r\n      \"energy\",\r\n      \"tempo\",\r\n      \"loudness\",\r\n      \"acousticness\",\r\n      \"danceability\"),\r\n    values_to = 'value') \r\n\r\nNow, we can visualize the results for each artist. As mentioned before, I will compare artists by the minimum (red), the average (orange), and maximum (yellow) values of each audio feature in their songs.\r\n\r\n\r\nShow code\r\nggplot() +\r\n  ### This plots the average of each audio feature\r\n  geom_polygon(\r\n    data = scaled_features_long %>%  group_by(artist_name, metric) %>%\r\n      summarise_at(c(\"value\"), mean) %>%\r\n      arrange(factor(metric, levels = order)) %>%\r\n      ungroup(),\r\n    aes(x = metric, y = value, group = artist_name,),\r\n    alpha = .54,\r\n    size = 1.5,\r\n    show.legend = T,\r\n    fill = '#FF1654'\r\n  ) +\r\n  ### This plots the maximum of each audio feature\r\n  geom_polygon(\r\n    data = scaled_features_long %>%  group_by(artist_name, metric) %>%\r\n      summarise_at(c(\"value\"), max) %>%\r\n      arrange(factor(metric, levels = order)) %>%\r\n      ungroup(),\r\n    aes(x = metric, y = value, group = artist_name,),\r\n    alpha = .44,\r\n    size = 1.5,\r\n    show.legend = T,\r\n    fill = '#FFE066'\r\n  ) +\r\n  ### This plots the mimumn of each audio feature\r\n  geom_polygon(\r\n    data = scaled_features_long %>%  group_by(artist_name, metric) %>%\r\n      summarise_at(c(\"value\"), min) %>%\r\n      arrange(factor(metric, levels = order)) %>%\r\n      ungroup(),\r\n    aes(x = metric, y = value, group = artist_name,),\r\n    alpha = .84,\r\n    size = 1.5,\r\n    show.legend = T,\r\n    fill =  \"#EF476F\"\r\n  ) +\r\n  scale_x_discrete(\r\n    limits = order,\r\n    labels = c(\r\n      \"Happy\",\r\n      \"Energy\",\r\n      \"Fast\",\r\n      \"Loud\",\r\n      \"Acoustic\",\r\n      \"Instrumental\",\r\n      \"Danceable\"\r\n    )\r\n  ) +\r\n  coord_polar(clip = 'off') +\r\n  theme_minimal() +\r\n  labs(title = \"Persian Singers and Their Audio Characteristics\",\r\n       caption = 'Source: Spotify \\n Visualization: mcnakhaee') +\r\n  ylim(0, 8) +\r\n  facet_wrap( ~ artist_name, ncol = 4) +\r\n  theme(\r\n    axis.title = element_blank(),\r\n    axis.ticks = element_blank(),\r\n    axis.text.y = element_blank(),\r\n    axis.text.x = element_text(\r\n      family =  'Montserrat',\r\n      size = 13.5,\r\n      margin = ggplot2::margin(30, 0, 20, 0)\r\n    ),\r\n    plot.caption = element_text(\r\n      family = 'Montserrat',\r\n      margin = ggplot2::margin(30, 0, 20, 0),\r\n      size = 11,\r\n      color = 'grey80'\r\n    ) ,\r\n    text = element_text(family =  'Montserrat'),\r\n    strip.text = element_text(family =  'Montserrat', size = 18),\r\n    strip.text.x = element_text(margin = ggplot2::margin(1, 1, 1, 1, \"cm\")),\r\n    panel.spacing = unit(3.5, \"lines\"),\r\n    panel.grid = element_blank(),\r\n    plot.title = element_text(\r\n      family = 'Montserrat',\r\n      hjust = .5,\r\n      margin = ggplot2::margin(30, 0, 20, 0),\r\n      size = 32,\r\n      color = 'gray10'\r\n    )\r\n  )\r\n\r\nLooking more closely at each audio feature\r\nMy first plot is informative, but it only gives us an overall picture of audio features. However, I would like to have a more detailed picture of singers and the audio features for each of their songs. For this reason, I will also make a separate plot for each audio feature where every song and its corresponding feature values are shown. I will also mark a few popular songs from each artist with a different color on this plot.\r\n\r\n\r\nShow code\r\n# Set a custom theme for our plots\r\ntheme_set(theme_void() +\r\n  theme(\r\n    text = element_text(family =  'Montserrat'),\r\n    axis.text.x = element_text(\r\n      family = 'Montserrat',\r\n      margin = ggplot2::margin(30, 0, 20, 0),\r\n      color = 'gray80',\r\n      size = 18\r\n    ),\r\n    axis.text.y = element_text(\r\n      family = 'Montserrat',\r\n      margin = ggplot2::margin(30, 0, 20, 20),\r\n      color = 'gray80',\r\n      size = 20\r\n    ),\r\n    axis.title.x = element_text(\r\n      family = 'Montserrat',\r\n      margin = ggplot2::margin(30, 0, 20, 0),\r\n      size = 22,\r\n      color = 'gray80'\r\n    ),\r\n    plot.title = element_text(\r\n      family = 'Montserrat',\r\n      hjust = .5,\r\n      margin = ggplot2::margin(40, 0, 40, 0),\r\n      size = 35,\r\n      color = 'gray80'\r\n    ),\r\n    plot.caption = element_text(family ='Montserrat',\r\n                                  margin = ggplot2::margin(30, 0, 20, 20),\r\n                                      size = 20,\r\n                                  color = 'gray70') ,\r\n    legend.position = 'none',\r\n    plot.background = element_rect(fill = \"#516869\")\r\n  ))\r\n\r\nAgain here, I will change the dataset to make it ready for visualization.\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop_jitter <- songs_audio_plus_pop %>% \r\n  filter(artist_name %in% artists) %>% \r\n  mutate(is_popular = !is.na(popularity)) %>%\r\n  distinct(artist_name,track_name,.keep_all = T) %>% \r\n  mutate(is_popular_size = if_else(!is.na(popularity),popularity,25),\r\n         is_popular_alpha = if_else(!is.na(popularity),0.8,0.5)) %>% \r\n  mutate(track_name= str_wrap(track_name, width = 15)) %>% \r\n  mutate(popular_track_name = if_else( !is.na(popularity) & nchar(track_name) < 20 & !explicit,track_name,'')) \r\n\r\nHappiness\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop_jitter %>%\r\n  ggplot(aes(x = artist_name, y = valence)) +\r\n  geom_jitter(\r\n    aes(\r\n      color = is_popular,\r\n      size = is_popular_size,\r\n      alpha = is_popular_alpha\r\n    ),\r\n    size = 6,\r\n    width = 0.2,\r\n  ) +\r\n  geom_text_repel(\r\n    aes(label = popular_track_name , x = artist_name , y = valence),\r\n    family = 'Montserrat',\r\n    color = 'gray99',\r\n    size = 5,\r\n    force = 0.6,\r\n    max.iter = 2000,\r\n    box.padding = 0.4,\r\n    point.padding = 0.6,\r\n    min.segment.length = 0.15,\r\n    nudge_y      = 0.001,\r\n    hjust = 0.5,\r\n    segment.alpha = 0.6,\r\n    segment.size = 0.6\r\n  ) +\r\n  stat_summary(\r\n    fun = mean,\r\n    geom = 'point',\r\n    color = '#FF9F1C',\r\n    size = 5,\r\n    aes(group = artist_name)) +\r\n  scale_color_manual(values = c('#FFD166', '#EF476F')) +\r\n  scale_y_continuous(sec.axis = dup_axis()) +\r\n  coord_flip()\r\n\r\n\r\nEnergy\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop_jitter %>%\r\n  ggplot(aes(x = artist_name, y = energy)) +\r\n  geom_jitter(\r\n    aes(\r\n      color = is_popular,\r\n      size = is_popular_size,\r\n      alpha = is_popular_alpha\r\n    ),\r\n    size = 6,\r\n    width = 0.2,\r\n  ) +\r\n  geom_text_repel(\r\n    aes(label = popular_track_name , x = artist_name , y = energy),\r\n    family = 'Montserrat',\r\n    color = 'gray90',\r\n    size = 6,\r\n    force = 0.6,\r\n    max.iter = 2000,\r\n    box.padding = 0.4,\r\n    point.padding = 0.6,\r\n    min.segment.length = 0.15,\r\n    nudge_y      = 0.001,\r\n    hjust = 0.5,\r\n    segment.alpha = 0.6,\r\n    segment.size = 0.6\r\n  ) +\r\n  stat_summary(\r\n    fun = mean,\r\n    geom = 'point',\r\n    color = '#FF9F1C',\r\n    size = 5,\r\n    aes(group = artist_name)\r\n  ) +\r\n  scale_color_manual(values = c('#EF476F', '#EF476F')) +\r\n  scale_y_continuous(sec.axis = dup_axis()) +\r\n  coord_flip() \r\n\r\n\r\nAcousticness\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop_jitter %>%\r\n  ggplot(aes(x = artist_name, y = acousticness)) +\r\n  geom_jitter(\r\n    aes(\r\n      color = is_popular,\r\n      size = is_popular_size,\r\n      alpha = is_popular_alpha\r\n    ),\r\n    size = 6,\r\n    width = 0.2,\r\n  ) +\r\n  geom_text_repel(\r\n    aes(label = popular_track_name , x = artist_name , y = acousticness),\r\n    family = 'Montserrat',\r\n    color = 'gray90',\r\n    size = 6,\r\n    force = 0.6,\r\n    max.iter = 2000,\r\n    box.padding = 0.4,\r\n    point.padding = 0.6,\r\n    min.segment.length = 0.15,\r\n    nudge_y      = 0.001,\r\n    hjust = 0.5,\r\n    segment.alpha = 0.6,\r\n    segment.size = 0.6\r\n  ) +\r\n  stat_summary(\r\n    fun = mean,\r\n    geom = 'point',\r\n    color = '#FF9F1C',\r\n    size = 5,\r\n    aes(group = artist_name)\r\n  ) +\r\n  scale_color_manual(values = c('#118AB2', '#06D6A0')) +\r\n  scale_y_continuous(sec.axis = dup_axis()) +\r\n  coord_flip() \r\n\r\n\r\nDanceability\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop_jitter %>%\r\n  ggplot(aes(x = artist_name, y = danceability)) +\r\n  geom_jitter(\r\n    aes(\r\n      color = is_popular,\r\n      size = is_popular_size,\r\n      alpha = is_popular_alpha\r\n    ),\r\n    size = 6,\r\n    width = 0.2,\r\n  ) +\r\n  geom_text_repel(\r\n    aes(label = popular_track_name , x = artist_name , y = danceability),\r\n    family = 'Montserrat',\r\n    color = 'gray90',\r\n    size = 6,\r\n    force = 0.6,\r\n    max.iter = 2000,\r\n    box.padding = 0.4,\r\n    point.padding = 0.6,\r\n    min.segment.length = 0.15,\r\n    nudge_y      = 0.001,\r\n    hjust = 0.5,\r\n    segment.alpha = 0.6,\r\n    segment.size = 0.6\r\n  ) +\r\n  stat_summary(\r\n    fun = mean,\r\n    geom = 'point',\r\n    color = '#FF9F1C',\r\n    size = 5,\r\n    aes(group = artist_name)\r\n  ) +\r\n  scale_color_manual(values = c('#A5668B', '#EF476F')) +\r\n  scale_y_continuous(sec.axis = dup_axis()) +\r\n  coord_flip()\r\n\r\n\r\nLoudness\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop_jitter %>%\r\n  ggplot(aes(x = artist_name, y = loudness)) +\r\n  geom_jitter(\r\n    aes(\r\n      color = is_popular,\r\n      size = is_popular_size,\r\n      alpha = is_popular_alpha\r\n    ),\r\n    size = 6,\r\n    width = 0.2,\r\n    \r\n  ) +\r\n  geom_text_repel(\r\n    aes(label = popular_track_name , x = artist_name , y = loudness),\r\n    family = 'Montserrat',\r\n    color = 'gray90',\r\n    size = 6,\r\n    force = 0.6,\r\n    max.iter = 2000,\r\n    box.padding = 0.4,\r\n    point.padding = 0.6,\r\n    min.segment.length = 0.15,\r\n    nudge_y      = 0.001,\r\n    hjust = 0.5,\r\n    segment.alpha = 0.6,\r\n    segment.size = 0.6\r\n  ) +\r\n  stat_summary(\r\n    fun = mean,\r\n    geom = 'point',\r\n    color = '#FF9F1C',\r\n    size = 5,\r\n    aes(group = artist_name)\r\n  ) +\r\n  scale_color_manual(values = c('#06D6A0', '#EF476F')) +\r\n  scale_y_continuous(sec.axis = dup_axis()) +\r\n  coord_flip() \r\n\r\n\r\nMost Popular Songs\r\nAs I mentioned previously, we can only retrieve his/her top 10 popular songs for each artist. The popularity of a track is a value between 0 (the least popular) and 100 (the most popular). Spotify uses an algorithm to calculate popularity scores, which is heavily influenced by the total number of times a song has been played recently. You can read more about it in this link.\r\nKnowing this fact about how popularity is measured, we can visualize songs and artists that have been popular and played recently.\r\n\r\n\r\nShow code\r\nsongs_audio_plus_pop <- songs_audio_plus_pop %>%\r\n  filter(\r\n    !artist_name %in% c(\r\n      'Hatam Asgari',\r\n      'Kaveh Deylami',\r\n      'Nasser Abdollahi',\r\n      'Peyman Yazdanian',\r\n      'Abbas Ghaderi',\r\n      'Mohammad Golriz',\r\n      'Hamid Hami',\r\n      'Koveyti Poor',\r\n      'Mohsen Sharifian',\r\n      'Soheil Nafissi'))\r\nsongs_audio_plus_pop %>%\r\n  filter(!is.na(popularity)) %>%\r\n  mutate(track_name = if_else(!is.na(track_name), track_name, track_name)) %>%\r\n  group_by(artist_name) %>%\r\n  summarize(\r\n    avg_pop = mean(popularity),\r\n    min_pop = min(popularity),\r\n    max_pop = max(popularity),\r\n    most_popular = track_name[which.max(popularity)],\r\n    least_popular = track_name[which.min(popularity)]\r\n  ) %>%\r\n  mutate(\r\n    artist_name = fct_reorder(artist_name, avg_pop),\r\n  ) %>%\r\n  \r\n  ggplot(aes(x = min_pop , xend = max_pop, y = artist_name)) +\r\n  geom_dumbbell(\r\n    colour_x = '#ef476f',\r\n    colour_xend = '#118ab2',\r\n    size_x = 7,\r\n    size_xend = 7\r\n  ) +\r\n  geom_text(\r\n    aes(x = min_pop - 1, y = artist_name, label = least_popular),\r\n    size = 7,\r\n    family = 'Montserrat',\r\n    hjust = 1\r\n  ) +\r\n  geom_text(\r\n    aes(x = max_pop + 1, y = artist_name, label = most_popular),\r\n    size = 7,\r\n    family = 'Montserrat',\r\n    hjust = 0\r\n  ) +\r\n  scale_x_continuous(sec.axis = dup_axis()) +\r\n  theme_tufte() +\r\n  theme(\r\n    plot.title = element_text(\r\n      family = 'Montserrat',\r\n      hjust = .5,\r\n      margin = ggplot2::margin(0, 0, 40, 0),\r\n      size = 45\r\n    ),\r\n    plot.subtitle = element_markdown(\r\n      family = 'Montserrat',\r\n      size = 15,\r\n      margin = ggplot2::margin(20, 0, 40, 0),\r\n      hjust = 1\r\n      \r\n    ),\r\n    axis.text.x = element_text(\r\n      family = 'Montserrat',\r\n      margin = ggplot2::margin(30, 0, 20, 0),\r\n      size = 20\r\n    ),\r\n    \r\n    axis.text.y = element_text(\r\n      family = 'Montserrat',\r\n      margin = ggplot2::margin(30, 0, 20, 0),\r\n      size = 20\r\n    ),\r\n    axis.title.x = element_text(\r\n      family = 'Montserrat',\r\n      margin = ggplot2::margin(30, 0, 20, 0),\r\n      size = 30\r\n    ),\r\n    plot.caption = element_text(family ='Montserrat',\r\n                                margin = ggplot2::margin(30, 0, 20, 20),\r\n                                size = 20,\r\n                                color = 'gray20') ,\r\n    axis.title.y = element_blank(),\r\n    plot.background = element_rect(fill = '#FCF0E1'),\r\n    plot.margin = unit(c(1, 1, 1.5, 1.2), \"cm\")\r\n  )\r\n\r\n\r\nThis plot shows the most popular song and the least popular track of each artist among his top 10 songs. The artists are also sorted based on their average popularity.\r\n\r\n\r\n",
    "preview": "posts/2020-05-04-persiansongs/featured.JPG",
    "last_modified": "2021-06-05T14:15:52+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-08-analyzing-the-2020-democratic-presidential-debates-part-2/",
    "title": "Analyzing the 2020 Democratic Presidential Debates - Part 2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2020-03-08",
    "categories": [
      "NLP",
      "R",
      "Election",
      "Spacy"
    ],
    "contents": "\r\n\r\nContents\r\n1. Introduction\r\n2. Workflow\r\n2.2 Loading the dataset\r\n2.1. Tokenization\r\n3. Named Entity Recognition using Spacy\r\n4. Sentiment Analysis\r\n\r\n5.Network Visualization\r\n5.2 Manual name correction\r\n5.3 Candidates interaction\r\n5.4 Organization and companies named entities\r\n5.5 The network of named entities for countries and cities\r\n5.6 The network of named entities for laws\r\n5.7 The network of named entities for nationalities, religious or political groups\r\n\r\n\r\n1. Introduction\r\nMany of us could not watch every 2020 Democratic Primary debate. It was important for some of us to know what happened during the debates. In my case, I was reading about what happened in debates in some online newspapers, or I watched a highlight of a debate on Youtube the next day. However, they only give a summary of a debate or just broadcast a portion of debates that includes a heated exchange of opinions between candidates. As a result, many important issues raised by candidates will be ignored and forgotten in the aftermath of a debate. So, it is crucial to summarize the debate’s content so that everyone could understand what went on in the debate and what issues each candidate addressed during his/her speech. In this blog post, I will show you how I used some NLP techniques for exploring the content of debates and give you a comprehensive overview of topics that each candidate discussed.\r\nIn my last blog post, I explained that I had the three following goals in mind when I started exploring the 2020 Democratic Debates :\r\nTo know how eloquent presidential candidates are.\r\nTo find out who used more positive or more negative words in his/her speech by performing sentiment analysis.\r\nA map of topics, individuals, and entities that each candidate mentioned in his/her speech by using named entity recognition and network analysis..\r\nI only discussed how I approached the first two aspects of my experiment in my last blogpost. Now it is time to investigate the third and last one.\r\nInitially, my aim was to use network analysis to determine potential allies and enemies on the debate stage. For example, Elizabeth Warren mentioned Mike Bloomberg several times and attacked him harshly in the 9th debate. During the same debate, Amy Klobuchar and Pete Buttigieg clashed bitterly with each other. These are just two instances of many other heated exchanges between the candidates that happened throughout the ten debates.\r\n\r\nTo make things more precise, I transformed my objective into two questions that I would like to answer:\r\n\r\n\r\nHow many times did a candidate address (mention) other candidates during a debate?\r\nHow did he/she refer to a candidate(in a friendly or unfriendly manner)?\r\n\r\nA simple approach to answering these questions is to store the names of all candidates in a variable (for example, a vector in R or a list in Python), iterate over the transcript, compute the sentiment, count and store the number of times that another candidate brought up a candidate’s name.\r\nHowever, this approach is a little bit challenging and requires a lot of manual data pre-processing efforts. For each democratic candidate, one must compile a comprehensive combination of ways that may be used to call a candidate, and to prepare such a list seems to be a very time-consuming task. For example, other candidates mentioned Bernie Sanders in many different ways, including Bernie, Bernie Sanders, or Senator Sanders.\r\nI realized that I could use Named Entity Recognition (NER), a technique from the Natural Language Processing (NLP) literature, to extract candidates’ names from the transcript and solve this problem more efficiently. Using this approach, I can find candidates’ names from the transcript, but I can also find the names of other politicians, individuals, and even organizations and further extend my analysis to include many more topics and issues.\r\n2. Workflow\r\nI made use of both Python and R in my analysis. My workflow includes the following steps:\r\nI access the transcript of debates using this package.\r\nI use tidytext to split the transcript into multiple sentences and also for sentiment analysis.\r\nI extract several types of Named Entities from each sentence, using Spacy,\r\nI compute the sentiment of each sentence using TextBlob library in Python.\r\nI transferred the results to R for visualization. There, I visualize the network of mentions and entities using ggraph and ggplot library.\r\nNote that I could have implemented all the steps in R. For instance, Spacy has an R wrapper called Spacyr, which gives the same functionality that I need for this analysis. However, I’d like to increase the number of tools that I can use. Notably, using Python and R side by side is an exciting challenge for me.\r\n\r\n\r\nShow code\r\nlibrary(demdebates2020)\r\nlibrary(tidytext)\r\nlibrary(tidygraph)\r\nlibrary(tidyverse)\r\nlibrary(ggraph)\r\nlibrary(gghighlight)\r\nlibrary(ggthemes)\r\nlibrary(kableExtra)\r\nlibrary(reticulate)\r\nlibrary(magrittr)\r\nlibrary(pluralize)\r\ntheme_set(\r\n  theme_graph(base_family = 'Montserrat')  +\r\n    theme(\r\n      panel.border = element_blank(),\r\n      plot.title = element_text(\r\n        family = 'Montserrat',\r\n        face = \"bold\",\r\n        colour = '#540b0e',\r\n        size = 42,\r\n        margin = ggplot2::margin(40, 40, 20, 10),\r\n        hjust = 0\r\n      ),\r\n      plot.subtitle =  element_text(\r\n        family = 'Montserrat',\r\n        face = \"bold\",\r\n        colour = '#7d4f50',\r\n        size = 30,\r\n        margin = ggplot2::margin(20, 40, 80, 10),\r\n        hjust = 0\r\n      ),\r\n      plot.caption =  element_text(\r\n        family = 'Montserrat',\r\n        face = \"bold\",\r\n        colour = '#540b0e',\r\n        size = 16,\r\n        margin = ggplot2::margin(0, 0, 20, 20),\r\n      ),\r\n      legend.position = 'none',\r\n      plot.background = element_rect(fill = '#FCF0E1'),\r\n      \r\n    )\r\n)\r\n\r\n2.2 Loading the dataset\r\n\r\n\r\nShow code\r\nhead(debates) \r\n\r\n\r\n\r\nShow code\r\nhead(debates) %>% \r\nkable() %>%\r\n kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\r\n\r\nspeaker\r\n\r\n\r\nbackground\r\n\r\n\r\nspeech\r\n\r\n\r\ntype\r\n\r\n\r\ngender\r\n\r\n\r\ndebate\r\n\r\n\r\nday\r\n\r\n\r\norder\r\n\r\n\r\nSavannah Guthrie\r\n\r\n\r\nNA\r\n\r\n\r\nAll right. So with that business out of the way, we want to get to it. And we’ll start this evening with Senator Elizabeth Warren.\r\n\r\n\r\nModerator\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nSavannah Guthrie\r\n\r\n\r\nNA\r\n\r\n\r\nSenator, good evening to you.\r\n\r\n\r\nModerator\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\nElizabeth Warren\r\n\r\n\r\nNA\r\n\r\n\r\nThank you. Good to be here.\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\nSavannah Guthrie\r\n\r\n\r\nNA\r\n\r\n\r\nYou have many plans - free college, free child care, government health care, cancellation of student debt, new taxes, new regulations, the breakup of major corporations. But this comes at a time when 71 percent of Americans say the economy is doing well, including 60 percent of Democrats. What do you say to those who worry this kind of significant change could be risky to the economy?\r\n\r\n\r\nModerator\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\nElizabeth Warren\r\n\r\n\r\nNA\r\n\r\n\r\nSo I think of it this way. Who is this economy really working for? It’s doing great for a thinner and thinner slice at the top. It’s doing great for giant drug companies. It’s just not doing great for people who are trying to get a prescription filled.\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\nElizabeth Warren\r\n\r\n\r\nNA\r\n\r\n\r\nIt’s doing great for people who want to invest in private prisons, just not for the African Americans and Latinx whose families are torn apart, whose lives are destroyed, and whose communities are ruined.\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n2.1. Tokenization\r\nAs I mentioned before, I use tidytext to tokenize the transcript dataset based on sentences. For sentence tokenization, you need to set token = 'sentences' in unnest_tokens() function. I think sentence tokenization is a reasonable choice because candidates might change the subject or the tone of their speech in each sentence.\r\n\r\n\r\nShow code\r\ndebates <- debates %>%\r\n unnest_tokens(sentence, speech, token = 'sentences',to_lower = FALSE)\r\n\r\n\r\n\r\nShow code\r\nhead(debates) \r\n\r\n\r\n\r\nShow code\r\nhead(debates) %>%\r\n kable() %>%\r\n kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\r\n\r\nspeaker\r\n\r\n\r\nbackground\r\n\r\n\r\ntype\r\n\r\n\r\ngender\r\n\r\n\r\ndebate\r\n\r\n\r\nday\r\n\r\n\r\norder\r\n\r\n\r\nsentence\r\n\r\n\r\nSavannah Guthrie\r\n\r\n\r\nNA\r\n\r\n\r\nModerator\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nAll right.\r\n\r\n\r\nSavannah Guthrie\r\n\r\n\r\nNA\r\n\r\n\r\nModerator\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nSo with that business out of the way, we want to get to it.\r\n\r\n\r\nSavannah Guthrie\r\n\r\n\r\nNA\r\n\r\n\r\nModerator\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nAnd we’ll start this evening with Senator Elizabeth Warren.\r\n\r\n\r\nSavannah Guthrie\r\n\r\n\r\nNA\r\n\r\n\r\nModerator\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\nSenator, good evening to you.\r\n\r\n\r\nElizabeth Warren\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\nThank you.\r\n\r\n\r\nElizabeth Warren\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\nGood to be here.\r\n\r\n\r\n3. Named Entity Recognition using Spacy\r\nNow we change to python for NER, but we need to install and import a few python libraries before starting the analysis.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\nimport spacy\r\nfrom textblob import TextBlob\r\n\r\nIn RStudio and Rmarkdown notebooks, with the help of the reticulate library, we can easily load the debate dataset in our R environment to our Python environment.\r\n\r\n\r\nShow code\r\ndebates = r.debates\r\n\r\nNote that in in the transcript dataset there are rows for both the candidates and the moderators who asked questions from candidates. However, we are particularly interested in what the candidates said, so we only filter rows corresponding to candidates.\r\n\r\n\r\nShow code\r\ncandidates = debates[(debates['type'] == 'Candidate') & (pd.notnull(debates['sentence'])) ]\r\n\r\nWe are almost ready to extract the named entities. However, to use Spacy’s NLP features such as NER, we first need to download and load a pre-trained English language model. There are several English language models with different sizes available in Spacy. I used the largest language model available as it might be better and more accurate.\r\n\r\n\r\nShow code\r\nnlp = spacy.load('en_core_web_lg')\r\n\r\nSpacy’s NER model is trained on the OntoNotes 5 corpus, and it can detect several types of named entities, including:\r\nTYPE\r\nDESCRIPTION\r\nPERSON\r\nPeople, including fictional.\r\nNORP\r\nNationalities or religious or political groups.\r\nFAC\r\nBuildings, airports, highways, bridges, etc.\r\nORG\r\nCompanies, agencies, institutions, etc.\r\nGPE\r\nCountries, cities, states.\r\nLOC\r\nNon-GPE locations, mountain ranges, bodies of water.\r\nPRODUCT\r\nObjects, vehicles, foods, etc. (Not services.)\r\nEVENT\r\nNamed hurricanes, battles, wars, sports events, etc.\r\nWORK_OF_ART\r\nTitles of books, songs, etc.\r\nLAW\r\nNamed documents made into laws.\r\nLANGUAGE\r\nAny named language.\r\nDATE\r\nAbsolute or relative dates or periods.\r\nTIME\r\nTimes smaller than a day.\r\nPERCENT\r\nPercentage, including ”%“.\r\nMONEY\r\nMonetary values, including unit.\r\nQUANTITY\r\nMeasurements, as of weight or distance.\r\nORDINAL\r\n“first”, “second”, etc.\r\nCARDINAL\r\nNumerals that do not fall under another type.\r\nAs you can see, there are many types of named entities, but I narrow down my analysis to just a handful of them, including PERSON, ORG, GPE, NORP, LAW, and LOC. The named entity labels are stored in label_ attribute. To do so, we need to create Doc object using nlp() method. When we call nlp() on the input text, spacy uses the language model to tokenize the document first. Then, spacy applies a tagger, parser, and named entity recognizer steps as its processing pipeline’s next components. The named entities can be accessed by ents attribute of the document object.\r\nIf you are interested to learn more about Spacy and how it works, I have provided some links at the end of this post.\r\nI define a python function that iterates over all named entities and see to which class of named entities (by default PERSON) they belong. I apply this function to the transcript column in the original dataset and store each extracted type of entity as a separate column.\r\n\r\n\r\nShow code\r\ndef extract_entities_delim(text,type_ent = 'PERSON'):\r\n  ent_text = ''\r\n  doc = nlp(text)\r\n  for e in doc.ents:\r\n    if e.label_ == type_ent:\r\n      ent_text = e.text+ ';' + ent_text \r\n  return ent_text\r\n\r\n\r\n\r\nShow code\r\ncandidates['PERSON'] = candidates['sentence'].apply(lambda x:extract_entities_delim(x))\r\ncandidates['ORG'] = candidates['sentence'].apply(lambda x:extract_entities_delim(x,'ORG'))\r\ncandidates['GPE'] = candidates['sentence'].apply(lambda x:extract_entities_delim(x,'GPE'))\r\ncandidates['NORP'] = candidates['sentence'].apply(lambda x:extract_entities_delim(x,'NORP'))\r\ncandidates['LAW'] = candidates['sentence'].apply(lambda x:extract_entities_delim(x,'LAW'))\r\ncandidates['LOC'] = candidates['sentence'].apply(lambda x:extract_entities_delim(x,'LOC'))\r\n\r\n4. Sentiment Analysis\r\nNext, I use TextBlob to compute each sentence’s sentiment and store its polarity score in a separate column called polarity_sentiment (TextBlob also returns a subjectivity score, but for simplicity, I will not use this score in my analysis). The polarity sentiment score is a value between -1 and 1. If the value is larger than 0, it means that the sentence has a positive sentiment. On the other hand, if the returned value is smaller than 0, it indicates that the sentence’s sentiment is negative.\r\n\r\n\r\nShow code\r\ndef polarity_sentiment(text):\r\n  blob = TextBlob(text)\r\n  return blob.sentiment.polarity\r\n  \r\ncandidates['polarity_sentiment'] = candidates.sentence.apply(lambda x:polarity_sentiment(x))\r\n\r\n5.Network Visualization\r\nA network (graph) can nicely represent how candidates mentioned individuals and entities in their speeches. We have two types of nodes in this network:\r\nThe first set of nodes represent candidates on the debate stage (from nodes).\r\nThe second set of nodes represent named entities (including the name of candidates themselves) that the candidates referred to in their speeches (to nodes).\r\nIf a candidate mentions a named entity in his/her speech, we connect the candidate node and the named entity node via our network’s edge. It is also fair to assume that the candidate-entity network should be weighted because candidates tend to place a varying level of importance on different issues, topics, and people (named entities).\r\nWe have two options for specifying weights for edges in the network:\r\nWe can use the number of times that a candidate mentioned a named entity in his/her speech. This shows how much a named entity was important to that candidate.\r\nWe can group by candidates and named entities and compute their average sentiment score. By doing so, we can measure how each candidate described these named entities. However, this approach might not be as accurate as we want.\r\nHaving said that, it is time to go back to R and visualize the network of candidates and named entities using the ggraph and tidygraph libraries. For each class of named entities, I use as_tbl_graph() function, create a unique graph table dataset, and visualize the network.\r\nSo, let us load the sentiment-entity dataset that I created in the Python environment to the R environment.\r\n\r\n\r\nShow code\r\ncandidates <- py$candidates\r\n\r\n\r\n\r\nShow code\r\ncandidates <- read_csv('candidates_sentiment_entities.csv')\r\n\r\n\r\n\r\nShow code\r\nhead(candidates) \r\n\r\n\r\n\r\nShow code\r\nhead(candidates) %>% \r\nkable() %>%\r\n kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\r\n\r\nX1\r\n\r\n\r\nspeaker\r\n\r\n\r\nbackground\r\n\r\n\r\ntype\r\n\r\n\r\ngender\r\n\r\n\r\ndebate\r\n\r\n\r\nday\r\n\r\n\r\norder\r\n\r\n\r\nsentence\r\n\r\n\r\nPERSON\r\n\r\n\r\nORG\r\n\r\n\r\nGPE\r\n\r\n\r\nNORP\r\n\r\n\r\nLAW\r\n\r\n\r\nLOC\r\n\r\n\r\npolarity_sentiment\r\n\r\n\r\n370\r\n\r\n\r\nAmy Klobuchar\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\nWell, first, the economy.\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n0.250\r\n\r\n\r\n371\r\n\r\n\r\nAmy Klobuchar\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\nWe know that not everyone is sharing in this prosperity.\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n0.000\r\n\r\n\r\n372\r\n\r\n\r\nAmy Klobuchar\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\nAnd Donald Trump just sits in the White House and gloats about what’s going on, when you have so many people that are having trouble affording college and having trouble affording their premiums.\r\n\r\n\r\nDonald Trump;\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n0.025\r\n\r\n\r\n373\r\n\r\n\r\nAmy Klobuchar\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\nSo I do get concerned about paying for college for rich kids.\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n0.375\r\n\r\n\r\n374\r\n\r\n\r\nAmy Klobuchar\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\nI do.\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n0.000\r\n\r\n\r\n375\r\n\r\n\r\nAmy Klobuchar\r\n\r\n\r\nNA\r\n\r\n\r\nCandidate\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\nBut I think my plan is a good one.\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n0.700\r\n\r\n\r\n5.1 The Candidate/Person Network\r\nFirst, I will visualize the candidate/person network. However, I should remind you that in the beginning of the democratic primary, many democratic candidates were competing against each other in the race and on the debate stage. If I were to visualize every individual that each candidate had ever in the network, the results would become unreadable. So, just like my last blog post, I selected a few democratic candidates to show my analysis.\r\nFurthermore, I will only highlight nodes corresponding to the top 6 democratic candidates and other interesting individuals, including Donald Trump and Barack Obama.\r\n\r\n\r\nShow code\r\ninteresting_individuals <-\r\n c(\r\n  \"Bernie Sanders\" ,\r\n  \"Elizabeth Warren\" ,\r\n  \"Mike Bloomberg\"  ,\r\n  \"Pete Buttigieg\" ,\r\n  \"Amy Klobuchar\" ,\r\n  \"Joe Biden\",\r\n  'Donald Trump',\r\n  'Barack Obama'\r\n )\r\n\r\n\r\n\r\ncustom_palette <-\r\n  c(\r\n    'Mike Bloomberg' = '#EDC948',\r\n    'Amy Klobuchar' = '#59A14F' ,\r\n    'Joe Biden' = '#E15759',\r\n    'Pete Buttigieg' = '#B07AA1',\r\n    'Elizabeth Warren' =  '#F28E2B',\r\n    'Bernie Sanders' =  '#4E79A7' ,\r\n    'Donald Trump' = '#BC3908',\r\n    'Barack Obama' = '#00afb9',\r\n    'Others' = '#540b0e'\r\n  )\r\n\r\n\r\n\r\nShow code\r\npersons_graph_table <- candidates %>%\r\n separate_rows(PERSON, sep = ';') %>%\r\n filter(speaker %in% interesting_individuals,PERSON != '', debate %in% c(8, 9, 10)) %>%\r\n mutate(from = speaker, to = PERSON) %>%\r\n group_by(from, to) %>%\r\n summarize(n_mentions = n(),\r\n      mean_sent = mean(polarity_sentiment),\r\n      sent =case_when(mean_sent < -0.01 ~ 'Negative',\r\n               mean_sent > 0.01 ~ 'Positive',\r\n              TRUE ~ 'Neutral' )\r\n               ) %>%\r\n ungroup() %>%\r\n as_tbl_graph() %>%\r\n mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, 'Others'))\r\n\r\n\r\n\r\nShow code\r\nedge_cols <- c('#e63946','#f1faee','#457B9D')\r\nggraph(persons_graph_table, layout = 'kk') + \r\n  geom_edge_link(aes(edge_width = n_mentions,colour = sent )) +\r\n  geom_node_point(aes(color = interesting_individuals ),size = 5) + \r\n  geom_node_label(aes(label = name,color = interesting_individuals),repel = TRUE,size= 8) + \r\n scale_color_manual(values = custom_palette) +\r\n  labs(title = 'Who Mentioned Whom in the 2020 Democratic Debates?') +\r\n  scale_edge_colour_manual(values = edge_cols) \r\n\r\n\r\nIf you look at the graph carefully, you will notice three issues with this network. First of all, the sentiment scores do not necessarily indicate how a candidate thinks about that person. For instance, Bernie Sanders and Khashoggi’s edge is red (i.e. negative sentiment), but Bernie Sanders did not talk negatively about Khashoggi at all but rather how he was murdered. Secondly, there are several nodes in the network that belong to the same individual. For example, Bernie Sanders tends to address other candidates by their first names, but other (younger) candidates usually use the last name to address each other.\r\nThe third issue is that some nodes do not represent a person. The transcript dataset is full of errors, and many names are misspelled. Although Spacy is a very powerful library for NER, sometimes it gives us wrong results, and its detected named entities are not always correct. For this reason, we also need to perform a post-processing step in which we remove some incorrectly spelled words or replaced them with their correct forms. I found two ways to deal with these issues: 1. or we can use a name matching algorithm to match the partial names with its full name. This approach can be challenging because we need to have a list of all possible full names, which is only available for the candidates.2. We can manually find undesirable names and replace them with what we want.\r\n5.1 Matching candidate names\r\nA python library called fuzzywuzzycan help us match two strings based on different similarity criteria. However, before using this library, I transform the original dataset into a long dataframe where each row belongs to a pair of candidate-person (from-to), and I move it back to our python environment.\r\n\r\n\r\nShow code\r\ncandidates_long <- candidates %>%\r\n filter(PERSON != '') %>%\r\n separate_rows(PERSON, sep = ';') \r\nhead(candidates_long)\r\nA tibble: 6 x 16\r\n X1 speaker  background type  gender debate   day order sentence  \r\n\r\n1 372 Amy Klo~ NA Cand~ female 1 1 11 And Donal~ 2 372 Amy Klo~ NA Cand~ female 1 1 11 And Donal~ 3 384 Amy Klo~ NA Cand~ female 1 1 99 It’s some~ 4 384 Amy Klo~ NA Cand~ female 1 1 99 It’s some~ 5 422 Amy Klo~ NA Cand~ female 1 1 329 But the p~ 6 422 Amy Klo~ NA Cand~ female 1 1 329 But the p~ # … with 7 more variables: PERSON , ORG , GPE , # NORP , LAW , LOC , polarity_sentiment \r\n\r\n\r\n\r\nShow code\r\ncandidates_long = r.candidates_long\r\n\r\nLet’s look at the full names of democratic candidates.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\ncandidate_lists = pd.unique(candidates_long.speaker)\r\nprint(candidate_lists)\r\n\r\nI define a python function called match_namesthat uses process.extractOne function to select the first matched named entity with at least 80 percent similarity to a candidate’s full name. The matched names are stored in a separate column called dem_candidate_full_name.\r\n\r\n\r\nShow code\r\nfrom fuzzywuzzy import fuzz \r\nfrom fuzzywuzzy import process \r\n\r\n\r\n\r\nShow code\r\ndef match_names(name):\r\n  try:\r\n    return process.extractOne(name, candidate_lists,score_cutoff = 80)[0] \r\n  except:\r\n    return None\r\n\r\ncandidates_long['dem_candidate_full_name'] = candidates_long.PERSON.apply(lambda x: match_names(x) )\r\n\r\nNow we can return to R and continue our analysis.\r\n\r\n\r\nShow code\r\ncandidates_long <- py$candidates_long\r\nglimpse(candidates_long)\r\n\r\nA word of caution\r\nWe need to be very careful with the results of the name-matching algorithm. There are too many politicians with the name ‘John’ and a John might refer to “John McCain” or “John Bolton” not the candidate “John Hickenlooper”. So, as a post-processing step, I manually explore the dataset to correct the few mistakes that the matching algorithm had made.\r\n\r\n\r\nShow code\r\ncandidates_long <- candidates_long %>%\r\n  mutate(PERSON = if_else(\r\n    !is.na(dem_candidate_full_name),\r\n    dem_candidate_full_name,\r\n    PERSON),\r\n  PERSON = case_when(PERSON == 'John Hickenlooper' & str_detect(sentence,'McCain') ~ 'John McCain',\r\n                      PERSON == 'John Hickenlooper' & str_detect(sentence,'Bolton') ~ 'John Bolton',\r\n                      PERSON == 'John Delaney' & speaker == 'Joe Biden' ~'John McCain',\r\n                      PERSON == 'John Delaney' & speaker == '   Amy Klobuchar' ~'John McCain',\r\n                      TRUE ~  PERSON)\r\n  ) \r\n\r\n5.2 Manual name correction\r\nWe have a better dataset now, but there are still a lot of inaccurate named entities or inconsistencies in the dataset. Let’s start by removing named entities that do not correspond with a real person.\r\n\r\n\r\nShow code\r\nnon_person <-\r\n  c(\r\n    'y adema' ,\r\n    'Appalachia' ,\r\n    'AUMF' ,\r\n    'bias',\r\n    'nondisclosur' ,\r\n    'Mathew 25',\r\n    'Idlib',\r\n    'ye',\r\n    'Everytown',\r\n    'Kurd',\r\n    'Roe V.',\r\n    'Wade',\r\n    'Trumpism',\r\n    'Casey',\r\n    'brown',\r\n    'Grandpa',\r\n    'Dad',\r\n    \"Josh\",\r\n    'Uighurs',\r\n    'Roe',\r\n    'PolitiFact',\r\n    'Latinx',\r\n    'Brady',\r\n    'pre-K.',\r\n    'Brady Bill',\r\n    'pro-Israel',\r\n    'ho',\r\n    'Dreamer'\r\n  )\r\n\r\nWe have another problem left. Some individuals were mentioned in different ways, and we have several nodes for them in the graph. To solve this issue, I use str_detect function from stringr package to manually modify them names. I must say this was the most tedious and time-consuming part of my analysis!\r\n\r\n\r\nShow code\r\npersons_graph_table <-  candidates_long %>%\r\n  filter(speaker %in% interesting_individuals,\r\n         !PERSON %in% non_person,\r\n         nchar(PERSON)>1) %>%\r\n  dplyr::rowwise() %>%\r\n  mutate(dem_candidate_full_name = as.character(dem_candidate_full_name)) %>%\r\n  mutate(from = speaker, to = PERSON) %>%\r\n  mutate(\r\n    to = case_when(\r\n      to %in% c(\r\n        'Donald',\r\n        'Donald Trump',\r\n        'Donald trump',\r\n        'Trump',\r\n        'President Trump',\r\n        \"Donald Trump's\"\r\n      ) ~ 'Donald Trump',\r\n      to %in% c('Hillar',\r\n                'Clinton',\r\n                'Hillary') ~ 'Hillary Clinton',\r\n      to %in% c('Obama',\r\n                'Barack') ~ 'Barack Obama',\r\n      str_detect(to, 'Trump') ~ 'Donald Trump',\r\n      str_detect(to, 'Vind') ~ 'Vindman',\r\n      str_detect(to, 'Assad') ~ 'Assad',\r\n      str_detect(to, 'McCarthy') ~ 'McCarthy',\r\n      str_detect(to, 'Trudeau') ~ 'Justin Trudeau',\r\n      str_detect(to, 'Bannon') ~ 'Steve Bannon',\r\n      str_detect(to, 'Netanyahu') ~ 'Netanyahu',\r\n      str_detect(to, 'Martin Luther') ~ 'Martin Luther King',\r\n      str_detect(to, 'Mandela') ~ 'Mandela',\r\n      str_detect(to, 'Xi') ~ 'Xi Jinping',\r\n      str_detect(to, 'Putin') ~ 'Putin',\r\n      str_detect(to, 'Mitch') ~ 'Mitch Mcconnell',\r\n      str_detect(to, 'Lindsey') ~ 'Lindsey Graham',\r\n      str_detect(to, 'Romney') ~ 'Mitt Romney',\r\n      str_detect(to, 'George') ~ 'George Bush',\r\n      str_detect(to, 'Bush') ~ 'George Bush',\r\n      str_detect(to, 'Turner') ~ 'Nina Turner',\r\n      str_detect(to, 'Clyburn') ~ 'Jim Clyburn',\r\n      str_detect(to, 'Cheney') ~ 'Dick Cheney',\r\n      str_detect(to, 'Shaheen') ~ 'Jeanne Shaheen',\r\n      str_detect(to, 'Hart') ~ 'Quentin Hart',\r\n      str_detect(to, 'Cokie') ~ 'Cokie Roberts',\r\n      str_detect(to, 'Kelly') ~ 'Laura Kelly',\r\n      str_detect(to, 'Berry') ~ 'Seth Berry',\r\n      str_detect(to, 'Grassley') ~ 'Chuck Grassley',\r\n      str_detect(to, 'Tommy') ~ 'Tom Steyer',\r\n      str_detect(to, 'Pelosi') ~ 'Nancy Pelosi',\r\n      str_detect(to, 'Kim') ~ 'Kim Jong-un',\r\n      str_detect(to, 'Pence') ~ 'Mike Pence',\r\n      str_detect(to, 'Schatz') ~ 'Brian Schatz',\r\n      str_detect(to, 'Gates') ~  'Robert Gates',\r\n      str_detect(to, 'Jill') ~ 'Jill Biden',\r\n      str_detect(to, 'Casey Jo') ~ 'Casey Jo',\r\n      str_detect(to, 'Franklin') |\r\n      str_detect(to, 'FDR') ~ 'Franklin D. Roosevelt',\r\n      str_detect(to, 'Welch') ~ 'Joseph Welch',\r\n      str_detect(to, 'Beau') ~ 'Beau Biden',\r\n      str_detect(to, 'Rudy Giuliani') ~ 'Rudy Giuliani',\r\n      str_detect(to, 'Bolton') ~ 'John Bolton',\r\n      str_detect(to, 'McCain') ~ 'John McCain',\r\n      str_detect(to, 'Truman') ~ 'Harry Truman',\r\n      str_detect(to, 'Dunford') ~ 'Joe Dunford',\r\n      str_detect(to, 'Breyer') ~ 'Justice Breyer',\r\n      str_detect(to, 'Cindy') ~ 'Cindy McCain',\r\n      to == 'Dick' ~ 'Uncle Dick',\r\n      to == 'Charles' ~ 'Charles Fried',\r\n      to == 'JFK' |\r\n        (to == 'Kennedy' & speaker == 'Joe Biden') ~ 'John F. Kennedy',\r\n      to == 'Kennedy' & speaker == 'Amy Klobuchar' ~ 'Ted Kennedy',\r\n      to %in% c('Joey') ~ 'Himself',\r\n      to %in% c(\r\n        'Ady',\r\n        'Carl',\r\n        'Ady Barkan',\r\n        'Derek',\r\n        'Mark',\r\n        'Salvador',\r\n        'Rachael',\r\n        'Nicole'\r\n      ) ~ 'American Constituents',\r\n      to %in% c(\r\n        'David',\r\n        'Chuck',\r\n        'Wolf',\r\n        '   Wolf',\r\n        'Margaret',\r\n        'Brianne',\r\n        'Adam',\r\n        'Jake',\r\n        'Norah',\r\n        'Judy',\r\n        'Gayle',\r\n        'Dana',\r\n        'Jorge - it',\r\n        'Lester',\r\n        'Rachel'\r\n      ) ~ 'Moderator',\r\n      TRUE ~ to\r\n    )\r\n  ) %>%\r\n  group_by(from, to) %>%\r\n  summarize(n_mentions = n()) %>%\r\n  ungroup() %>%\r\n  as_tbl_graph() %>%\r\n  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, 'Others'))  \r\n\r\nFinally, we can visualize the network with modified node names.\r\n\r\n\r\nShow code\r\nedge_cols <- c('#e63946', '#f1faee', '#457B9D')\r\nggraph(persons_graph_table, layout = 'nicely') +\r\n  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), color = '#540b0e') +\r\n  geom_node_point(aes(color = interesting_individuals),size = 6) +\r\n  geom_node_label(\r\n    aes(label = name, color = interesting_individuals),\r\n    repel = TRUE,\r\n    size = 8,\r\n    label.r = 0.4,\r\n    check_overlap = TRUE\r\n  ) +\r\n  scale_color_manual(values = custom_palette) +\r\n  labs(title = 'Individuals Mentioned by Top Democratic Candidates During the Democratic Primary Debates',\r\n       subtitle = 'This graph shows which individuals or politicians were mentioned by top 6 democratic candidates over the course of first ten priamary debates.',\r\n       caption = 'Visualization: @m_cnakhaee\\n\\n Source: https://github.com/favstats/demdebates2020') \r\n\r\n\r\n5.3 Candidates interaction\r\nIn the last sections, I explained how the top 6 remaining candidates mentioned other individuals during their speeches on the debate stage. However, with a little bit of modification to our previous chunk of code, we can extend the analysis and investigate how all democratic candidates interacted with each other over the course of 10 debates.\r\n\r\n\r\nShow code\r\n#The name of all candidaes\r\nall_candidates <- candidates_long %>%\r\n  distinct(speaker) %>%\r\n  pull()\r\n\r\ncandidates_graph_table <- candidates_long %>%\r\n  filter(!is.na(dem_candidate_full_name),\r\n         dem_candidate_full_name == PERSON) %>%\r\n  rowwise() %>%\r\n  mutate(debate = as.factor(debate)) %>%\r\n  mutate(dem_candidate_full_name = as.character(dem_candidate_full_name)) %>%\r\n  mutate(from = speaker, to = dem_candidate_full_name) %>%\r\n  group_by(from, to, debate) %>%\r\n  summarize(n_mentions = n()) %>%\r\n  ungroup() %>%\r\n  as_tbl_graph(directed = TRUE) %>%\r\n  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, 'Others')) %>%\r\n  activate(nodes) %>%\r\n  mutate(bet_cent = centrality_betweenness(),\r\n         deg_cent = centrality_degree())\r\n\r\n\r\n\r\nShow code\r\n# preparing the circular layout for the network\r\n# Credit to https://www.timlrx.com/2018/10/14/visualising-networks-in-asoiaf-part-ii/ for helping me with the circular layout\r\nfull_layout <-\r\n  create_layout(graph = candidates_graph_table,\r\n                layout = \"linear\",\r\n                circular = T)\r\n\r\nxmin <- min(full_layout$x)\r\nxmax <- max(full_layout$x)\r\nymin <- min(full_layout$y)\r\nymax <- max(full_layout$y)\r\n\r\nggraph(\r\n  full_layout,\r\n  layout = 'manual',\r\n  x = x,\r\n  y = y,\r\n  circular = TRUE\r\n) +\r\n  geom_edge_arc(aes(edge_width = n_mentions,\r\n                    alpha = n_mentions,),\r\n                colour = '#540b0e',) +\r\n  geom_node_point(aes(color = interesting_individuals, size = deg_cent + 40)) +\r\n  geom_node_text(\r\n    aes(\r\n      label = name,\r\n      color = interesting_individuals,\r\n      x = x * 1.15,\r\n      y = y * 1.15,\r\n      angle = ifelse(\r\n        atan(-(x / y)) * (180 / pi) < 0,\r\n        90 + atan(-(x / y)) * (180 / pi),\r\n        270 + atan(-x / y) * (180 / pi)\r\n      )\r\n    ),\r\n    size = 8\r\n  ) +\r\n  scale_color_manual(values = custom_palette) +\r\n  labs(title = 'The Network of Interactions Among Democratic Candidates During Democratic Primary Debates',\r\n       #subtitle = 'This graph shows how democtratic candidates mentioned other candidates on the debate stage.',\r\n       caption = 'Visualization: @m_cnakhaee\\n\\n Source: https://github.com/favstats/demdebates2020') +\r\n  expand_limits(x = c(xmin - 0.2, xmax + 0.2),\r\n                y = c(ymin - 0.2, ymax + 0.2)) \r\n\r\n\r\nThe results are self-explanatory and satisfying. One also can make an animation and show the network over time.\r\nNow, let’s repeat the same steps and visualize the network for other types of named entities.\r\n5.4 Organization and companies named entities\r\n\r\n\r\nShow code\r\nnon_org <- c('Trump','Vindmen','a New Yorker','Title','Obama',\"Donald Trump's\",'Bernie','state','Court','Ours','Education')\r\nnon_org_laws <- c('Green New Deal','Federal Controlled Substance Act')\r\n\r\norg_graph_table <- candidates %>%\r\n  separate_rows(ORG, sep = ';') %>%\r\n  filter(!is.na(ORG)) %>%\r\n  mutate(ORG = str_remove_all(ORG, 'the '),\r\n         ORG = str_remove_all(ORG, 'this '),) %>%\r\n  filter(\r\n    \r\n    speaker %in% interesting_individuals,!ORG %in% non_org,!ORG %in% non_org_laws,\r\n    debate %in% c(6, 7, 8, 9, 10),\r\n    nchar(ORG)>1\r\n  ) %>%\r\n  mutate(from = speaker, to = ORG) %>%\r\n  group_by(from, to) %>%\r\n  summarize(n_mentions = n()) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    to = case_when(\r\n      to %in% c('United Nations',\r\n                'U.N.',\r\n                'UN') ~ 'United Nations',\r\n      str_detect(to, 'Department') &\r\n        str_detect(to, 'State') ~ 'Department of State',\r\n      str_detect(to, 'Department') &\r\n        str_detect(to, 'Defence') ~ 'Department of State',\r\n      str_detect(to, 'Supreme') &\r\n        str_detect(to, 'Court') ~ 'Supreme Court',\r\n      str_detect(to, 'Treasury') ~ 'Department of the Treasury',\r\n      str_detect(to, 'Unitetd') &\r\n        str_detect(to, 'State') ~ 'United State',\r\n      str_detect(to, 'Yale') ~ 'Yale',\r\n      TRUE ~ to\r\n    )\r\n  ) %>%\r\n  as_tbl_graph() %>%\r\n  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, 'Others'))\r\n\r\n\r\n\r\nShow code\r\nggraph(org_graph_table, layout = 'nicely') +\r\n  geom_edge_link(aes(edge_width = n_mentions,alpha=n_mentions),\r\n    colour = '#540b0e') +\r\n  geom_node_point(aes(color = interesting_individuals), size = 5) +\r\n  geom_node_label(aes(label = name, color = interesting_individuals),\r\n                  repel = TRUE,\r\n                  size = 7) +\r\n  scale_color_manual(values = custom_palette) +\r\n  labs(title = 'Organizations and Institutions Mentioned by Top Democratic Candidates During the Debates',\r\n       subtitle = 'This plot shows which organizations and institutions were mentioned by top democratic candidates during the last 5 debates.',\r\n       caption = '') \r\n\r\n\r\n5.5 The network of named entities for countries and cities\r\n\r\n\r\nShow code\r\ngpe_graph_table <- candidates %>%\r\n  separate_rows(GPE, sep = ';') %>%\r\n  filter(!is.na(GPE)) %>%\r\n  mutate(GPE = str_remove_all(GPE, 'the '),\r\n         GPE = str_remove_all(GPE, 'this ')) %>%\r\n  filter(speaker %in% interesting_individuals,\r\n         debate %in% c(6, 7, 8, 9, 10)) %>%\r\n  mutate(from = speaker, to = GPE) %>%\r\n  group_by(from, to) %>%\r\n  summarize(n_mentions = n()) %>%\r\n  ungroup() %>%\r\n  as_tbl_graph() %>%\r\n  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, 'Others')) \r\n\r\n\r\n\r\nShow code\r\nggraph(gpe_graph_table, layout = 'nicely') +\r\n  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions),\r\n                 colour = '#540b0e') +\r\n  geom_node_point(aes(color = interesting_individuals), size = 5) +\r\n  geom_node_label(aes(label = name, color = interesting_individuals),\r\n                  repel = TRUE,\r\n                  size = 9) +\r\n  scale_color_manual(values = custom_palette) +\r\n  labs(title = 'Countries, Cities and States Mentioned by Top Democratic Candidates During the Last Five Primary Debates',\r\n       subtitle = '',\r\n       caption = '') \r\n\r\n\r\n5.6 The network of named entities for laws\r\n\r\n\r\nShow code\r\nnon_law <-\r\n  c('the ZIP Code', \"\")\r\n\r\nlaw_graph_table <- candidates %>%\r\n  separate_rows(LAW, sep = ';') %>%\r\n  filter(!is.na(LAW)) %>%\r\n  mutate(LAW = str_remove_all(LAW, 'the ')) %>%\r\n  filter(speaker %in% interesting_individuals,!LAW %in% non_law) %>%\r\n  mutate(from = speaker, to = LAW) %>%\r\n  group_by(from, to) %>%\r\n  summarize(n_mentions = n()) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    to = case_when(\r\n      str_detect(to , 'Constitution') ~ 'Constitution',\r\n      str_detect(to , 'Roe') ~ 'Roe V. Wade',\r\n      str_detect(to , 'War Powers Act') ~ 'War Powers Act',\r\n      str_detect(to , 'New START') ~ 'New START Treaty',\r\n      TRUE ~ to\r\n    )\r\n  ) %>%\r\n  as_tbl_graph() %>%\r\n  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, 'Others'))\r\n\r\n\r\n\r\nShow code\r\nggraph(law_graph_table, layout = 'nicely') +\r\n  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), colour = '#540b0e') +\r\n  geom_node_point(aes(color = interesting_individuals), size = 5) +\r\n  geom_node_label(aes(label = name, color = interesting_individuals),\r\n                  repel = TRUE,\r\n                  size = 7) +\r\n  scale_color_manual(values = custom_palette) +\r\n  labs(title = 'Laws Mentioned by Top Democratic Candidates During the First Ten Primary Debates',\r\n       subtitle = '',\r\n       caption = '') \r\n\r\n\r\n5.7 The network of named entities for nationalities, religious or political groups\r\n\r\n\r\nShow code\r\nnon_norp <- c('Coronavirus', '')\r\nnorp_graph_table <- candidates %>%\r\n  separate_rows(NORP, sep = ';') %>%\r\n  filter(!is.na(NORP)) %>%\r\n  mutate(NORP = singularize(NORP)) %>%\r\n  filter(speaker %in% interesting_individuals,!NORP %in% non_norp,\r\n         debate %in% c(6, 7, 8, 9, 10)) %>%\r\n  mutate(from = speaker, to = NORP) %>%\r\n  group_by(from, to) %>%\r\n  summarize(n_mentions = n()) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    to = case_when(\r\n      str_detect(to, 'African') &\r\n        str_detect(to, 'American') ~ 'African-American',\r\n      str_detect(to, 'republican') ~ 'Republican',\r\n      str_detect(to, 'Democrat') ~ 'Democrat',\r\n      str_detect(to, 'Jew') ~ 'Jew',\r\n      str_detect(to, 'Palestinian') ~ 'Palestinian',\r\n      TRUE ~ to\r\n    )\r\n  ) %>%\r\n  as_tbl_graph() %>%\r\n  mutate(interesting_individuals = if_else(name %in% interesting_individuals, name, 'Others'))\r\n\r\n\r\n\r\nShow code\r\nggraph(norp_graph_table, layout = 'nicely') +\r\n  geom_edge_link(aes(edge_width = n_mentions, alpha = n_mentions), colour = '#540b0e') +\r\n  geom_node_point(aes(color = interesting_individuals), size = 5) +\r\n  geom_node_label(aes(label = name, color = interesting_individuals),\r\n                  repel = TRUE,\r\n                  size = 10) +\r\n  scale_color_manual(values = custom_palette) +\r\n  labs(title = 'Nationalities, religious or Political Groups Mentioned by Top Democratic Candidates During the Last Five Primary Debates',\r\n       subtitle = '',\r\n       caption = '') \r\n\r\n\r\n##Resources: A very useful place to learn more how spacy works the spacy’s online course by one of its founders and developers. [1] https://course.spacy.io/en/\r\n[2] https://www.youtube.com/watch?v=IqOJU1-_Fi0&t=676s\r\nThere is http://www.favstats.eu/post/demdebates/\r\n\r\n\r\n",
    "preview": "posts/2020-03-08-analyzing-the-2020-democratic-presidential-debates-part-2/featured.JPG",
    "last_modified": "2021-06-06T15:04:56+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-01-spoitfymaps/",
    "title": "The Map of Spotify Songs",
    "description": "Mapping high dimensional audio features from Spotify's into a 2D space using UMPAN and TSNE algorithms.",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2020-02-01",
    "categories": [
      "R",
      "Machine Learning",
      "Dimensionaliy Reduction"
    ],
    "contents": "\r\n\r\nContents\r\nDimensionality Reduction and UMAP\r\nData Preprocessing\r\n\r\nT-SNE\r\nUMAP\r\nSetting 1\r\nSetting 2\r\nSetting 3\r\nSetting 4\r\nSetting 5\r\nSetting 6\r\n\r\nSupervised UMAP\r\n\r\nIn the 4th week of the Tidy Tuesday project, a very interesting and fun dataset was proposed to the data science community. The dataset contains information about thousands of songs on Spotify’s platform and along with their metadata and audio features. You can download the dataset can using the following piece of code.\r\n4th week of the Tidy Tuesday project\r\n\r\n\r\nShow code\r\nspotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\r\nhead(spotify_songs)\r\n# A tibble: 6 x 23\r\n  track_id  track_name    track_artist track_popularity track_album_id\r\n  <chr>     <chr>         <chr>                   <dbl> <chr>         \r\n1 6f807x0i~ I Don't Care~ Ed Sheeran                 66 2oCs0DGTsRO98~\r\n2 0r7CVbZT~ Memories - D~ Maroon 5                   67 63rPSO264uRjW~\r\n3 1z1Hg7Vb~ All the Time~ Zara Larsson               70 1HoSmj2eLcsrR~\r\n4 75Fpbthr~ Call You Min~ The Chainsm~               60 1nqYsOef1yKKu~\r\n5 1e8PAfcK~ Someone You ~ Lewis Capal~               69 7m7vv9wlQ4i0L~\r\n6 7fvUMiya~ Beautiful Pe~ Ed Sheeran                 67 2yiy9cd2QktrN~\r\n# ... with 18 more variables: track_album_name <chr>,\r\n#   track_album_release_date <chr>, playlist_name <chr>,\r\n#   playlist_id <chr>, playlist_genre <chr>, playlist_subgenre <chr>,\r\n#   danceability <dbl>, energy <dbl>, key <dbl>, loudness <dbl>,\r\n#   mode <dbl>, speechiness <dbl>, acousticness <dbl>,\r\n#   instrumentalness <dbl>, liveness <dbl>, valence <dbl>,\r\n#   tempo <dbl>, duration_ms <dbl>\r\n\r\nFor this week’s tidy Tuesday, I decided to use a somewhat different approach from my previous submissions. Instead of focusing solely on the visualization aspect of my submissions, I tried to use other tools from the tidy model universe for machine learning model development,\r\nEach song has around 12 columns representing audio features. The Github’s page for this dataset describes these features as follows:\r\nvariable\r\nclass\r\ndescription\r\ndanceability\r\ndouble\r\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\r\nenergy\r\ndouble\r\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\r\nkey\r\ndouble\r\nThe estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\r\nloudness\r\ndouble\r\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\r\nmode\r\ndouble\r\nMode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\r\nspeechiness\r\ndouble\r\nSpeechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\r\nacousticness\r\ndouble\r\nA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\r\ninstrumentalness\r\ndouble\r\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\r\nliveness\r\ndouble\r\nDetects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\r\nvalence\r\ndouble\r\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\r\ntempo\r\ndouble\r\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\r\nduration_ms\r\ndouble\r\nDuration of song in milliseconds\r\nIt would be very helpful to compare songs based on their audio features and have an overall picture of where each song is placed. Unfortunately, we can only visualize 2 or 3 audio features at the same time, and It is not possible to put all these features in a 2D or 3D space. So, I tried to use unsupervised machine learning to visualize songs on a 2D space by transforming their high-dimensional audio features into a more compressed form.\r\n\r\n\r\nShow code\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\nlibrary(workflows)\r\nlibrary(gghighlight)\r\nlibrary(hrbrthemes)\r\nlibrary(ggthemes)\r\nlibrary(lubridate)\r\nlibrary(reticulate)\r\nlibrary(ggrepel)\r\nlibrary(plotly)\r\nlibrary(uwot)\r\n\r\n\r\ntheme_update(legend.position = 'top',\r\n   legend.text  = element_text(size = 32,color = 'gray75' ),\r\n   legend.key = element_rect(fill = \"black\", color = \"black\"),\r\n   legend.background= element_rect(fill = \"black\", color = \"black\"),\r\n   plot.title = element_text(family = 'Montserrat', face = \"bold\", size = 60,hjust = 0.5,vjust = 0.5,color = '#FFE66D',margin = ggplot2::margin(40,0,0,0)),\r\n   plot.subtitle = element_text(\r\n   family = 'Montserrat', size = 30, hjust = 0.5),\r\n   strip.background = element_blank(),\r\n   plot.background = element_rect(fill = \"black\", color = \"black\"),\r\n   panel.background = element_rect(fill = \"black\", color = \"black\"),\r\n   panel.grid.major.x =element_blank(),\r\n   panel.grid.major.y =element_blank(),\r\n   panel.grid.minor =element_blank(),\r\n   axis.text.x.bottom = element_blank(),\r\n   axis.ticks.x = element_blank(), \r\n   axis.ticks.y = element_blank(),\r\n   axis.text.x = element_blank(),\r\n   axis.text.y.left = element_blank()) \r\n\r\nDimensionality Reduction and UMAP\r\nMy initial idea was to use some clustering algorithms to cluster songs based on their audio feature and find songs that are similar to each other. Yet, it was not easy to visualize these clusters in a two-dimensional space. Of course, you can do that by using hierarchal clustering but even then, visualizing a few thousand samples (songs) seems to be impractical. So, I decided to use other unsupervised techniques to compress these high-dimensional audio features and transform them into a more compact 2D space.\r\nThere are several dimensionality reduction algorithms such as PCA, t-SNE UMAP. The primary purpose of these algorithms is to give us a compressed representation of the input data, while preserving the most relevant information in the data. PCA is a linear dimensionality reduction method, while both t-SNE and UMAP are non-linear methods.\r\nIn this post, I will use UMAP and t-SNE, two widely used dimensionality reduction algorithms. When the input dataset is large T-SNE becomes very slow and is not an efficient algorithm anymore. On the other hand, UMAP can handle larger datasets much more easily and quickly. Moreover, UMAP can preserve the underlying local structure present in the data, and it can also represent the global structure of the data more accurately. What do we mean by local and global structure? For example, in the song dataset, persevering local structure means that songs that belong to an artist are clustered together. Similarly, global structure means that songs belonging to more related genres (e.g., hard rock, album rock, and classic rock) will be placed in close proximity to each other on the new projection.\r\nUMAP achieves this goal by employing some advanced optimization techniques and mathematical concepts. Understanding how UMAP uses these techniques and projects the input data into a more compressed representation is not crucial, but If you are curious to know more about the theory behind UMAP and its difference with T-SNE, I recommend this excellent blogpost by Andy Coenen and Adam Pearce.\r\nData Preprocessing\r\nBoth UMAP and T-SNE compute a distance metric between samples. This distance metric should be meaningful and reasonable. If we do not scale the input features before feeding them to these algorithms, some features might have a stronger (unfair) influence than other features on the computation of the distance between samples. For this reason, it is necessary to normalize input features before implementing them,\r\nI create a data preprocessing recipe using the recipe package, and I add a normalization step to scale the audio features. Note that since I implement an unsupervised algorithm, there is no need to split the dataset into a training and testing dataset.\r\n\r\n\r\nShow code\r\nnormalized_features <- spotify_songs %>%\r\n recipe() %>% \r\n step_normalize( danceability,\r\n  energy,\r\n  key,\r\n  loudness,\r\n  mode,\r\n  speechiness,\r\n  acousticness,\r\n  instrumentalness,\r\n  liveness,\r\n  valence,\r\n  tempo,\r\n  duration_ms) %>% \r\n prep() %>% \r\n juice()\r\n\r\nhead(normalized_features)\r\n# A tibble: 6 x 23\r\n  track_id  track_name    track_artist track_popularity track_album_id\r\n  <fct>     <fct>         <fct>                   <dbl> <fct>         \r\n1 6f807x0i~ I Don't Care~ Ed Sheeran                 66 2oCs0DGTsRO98~\r\n2 0r7CVbZT~ Memories - D~ Maroon 5                   67 63rPSO264uRjW~\r\n3 1z1Hg7Vb~ All the Time~ Zara Larsson               70 1HoSmj2eLcsrR~\r\n4 75Fpbthr~ Call You Min~ The Chainsm~               60 1nqYsOef1yKKu~\r\n5 1e8PAfcK~ Someone You ~ Lewis Capal~               69 7m7vv9wlQ4i0L~\r\n6 7fvUMiya~ Beautiful Pe~ Ed Sheeran                 67 2yiy9cd2QktrN~\r\n# ... with 18 more variables: track_album_name <fct>,\r\n#   track_album_release_date <fct>, playlist_name <fct>,\r\n#   playlist_id <fct>, playlist_genre <fct>, playlist_subgenre <fct>,\r\n#   danceability <dbl>, energy <dbl>, key <dbl>, loudness <dbl>,\r\n#   mode <dbl>, speechiness <dbl>, acousticness <dbl>,\r\n#   instrumentalness <dbl>, liveness <dbl>, valence <dbl>,\r\n#   tempo <dbl>, duration_ms <dbl>\r\n\r\nT-SNE\r\nBoth UMAP and T-SNE have several hyper-parameters that can influence the resulting embedding output. However, T-SNE is a notoriously slow algorithm and the opportunity for trial and error with different sets of hyper-parameter values are limited. For the sake of simplicity, I stick to default settings for hyper-parameter in T-SNE.\r\n\r\n\r\nShow code\r\nlibrary(Rtsne)\r\ntsne_embedding <- normalized_features %>%\r\n select(c(12:23)) %>%\r\n Rtsne(check_duplicates = FALSE)\r\n\r\ntsne_embeddings <- spotify_songs %>% \r\n select(-c(12:22)) %>% \r\n bind_cols(tsne_embedding$Y %>% as_tibble()) %>% = element_rect(fill = \"black\", color = \"black\"),\r\n dplyr::rename(tsne_1 = V1, tsne_2 = V2) %>% \r\n\r\nEven though I managed to transform a high dimensional dataset into a 2D space, still it was very challenging to visualize every song and every artists all at once. So, I just select a few famous artists that I have heard about. Each artist in this list more or less represents at least a genre of music and it can perfectly show that an artist (or a band) made several genres of music and how difficult our task is.\r\n\r\n\r\nShow code\r\nselected_artists <- c('Queen','Drake','Rihanna','Taylor Swift','Eminem','Snoop Dogg','Katy Perry','The Beatles')\r\n\r\n\r\n\r\nShow code\r\ntsne_embeddings <- tsne_embeddings%>% \r\n mutate(\r\n  selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), \"\"),\r\n  track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),\r\n  genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),\r\n  popular_tracks_selected_artist = if_else(\r\n   track_artist %in% selected_artists & track_popularity > 65,shorter_names, NULL )) %>%\r\n distinct(track_name, .keep_all = TRUE)\r\n\r\n\r\n\r\nShow code\r\ntsne_embeddings %>%\r\n ggplot(aes(x = tsne_1, y = tsne_2 ,color = selected_artist )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size = 0.8, color = '#FFE66D')) +\r\n scale_color_manual(values = c('#5BC0EB','#FDE74C','#7FB800','#E55934','#FA7921','#1A936F' ,'#F0A6CA','#B8BEDD'))+\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    title = 'The Map of Spotify Songs Based on T-SNE Algorithm\\n',\r\n    subtitle = 'Using the T-SNE algorithm, the audio features of each song are mapped into a 2D space.\\n Each point represents a unique song and the most popular songs of several known artist are also shown\\n',\r\n    color = '') \r\n\r\n\r\nAs you can see in this projection, songs that belong to the same artists are placed close to each other. It seems that T-SNE is able to preserve the local topological structure of songs. Now I will look at how T-SNE distinguishes different genres of music.\r\n\r\n\r\nShow code\r\ntsne_embeddings %>%\r\n ggplot(aes(x = tsne_1, y = tsne_2 ,color = playlist_genre )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size = 0.8, color = '#FFE66D')) +\r\n scale_color_tableau() +\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    title = 'The Map of Spotify Songs Based on T-SNE Algorithm\\n',\r\n    subtitle = 'Using the T-SNE algorithm, the audio features of each song are mapped into a 2D space.\\n Each point represents a unique song and the most popular songs of several known artist are also shown\\n',\r\n    color = '') \r\n\r\n\r\nUMAP\r\nJust like t-SNE, UMAP is a dimensionality reduction algorithm but it is much more computationally efficient and faster that t-SNE. The UMAP algorithm was originally implemented in Python. But there are also several libraries in R such as umapr, umap and uwot that also provide an implementation of the UMAP algorithm. umapr and umap use the reticulate package and provide a wrapper function around the original umap-learn python library. Also, umap and uwot library have their own R implementation and they do not require the python package to be installed beforehand. For this specific experiment, I will use the uwot library.\r\nwe can change and tune a few hyper-parameters in the implementation of UMAP in the uwot library, These hyperparameter can change the embedding outcome. However, there are two hyper-parameters that have a much more important impact on the structure of the low-dimensional representation:n_neighbors, min_dist and metric.\r\nn_neighbors determines the number of nearest neighbor data points that we use to compute and construct the embedding.\r\nmin_dist controls the minimum distance between data points in the low dimensional space (embedding). That means a low value of min_dist results in a more compact clusters of data points. On the other hand, with larger values of min_dist, the projection will be less compact and tend to preserve the global structure.\r\nmetric: We can use different metrics (e.g.. cosine or Euclidean) to compute the distance between data points and to find the nearest neighbors.\r\nThe choice of hyperparameter values can be very important for the final projection. However,choosing the right set of hyper-parameters in UMAP is extremely difficult because UMAP is an unsupervised algorithm and we do not have a baseline to evaluate its performance. Fortunately, UMAP is vary fast and scalable algorithm. It means that we can run UMAP with different hyperparameter settings and decide which set of values best serves our purpose.\r\nMy main goal from running UMAP is to visualize songs and their audio features on a 2D space and I can use a trick to decrease UMAP’s computation time. According to uwot’s documentation, if my only purpose is visualization, I can set the value of fast_sgd hyper-parameter to TRUE to speed up UMAP’s convergence and running time. Next, I create a grid of values for these three hyper-parameters and each time I will learn a new UMAP embedding based on different combinations of these values.\r\n\r\n\r\nShow code\r\nn_neighbors <- c(15,30,50,100,150)\r\nmin_distance <- c( 0.001, 0.003, 0.009,0.03,0.09)\r\nmetrics <- c(\"euclidean\" ,\"cosine\",\"hamming\")\r\n\r\n#make a copy of the dataset\r\nspotify_songs_emb <- spotify_songs\r\n\r\nfor (nn in n_neighbors) {\r\n for (md in min_distance) {\r\n  for (metric in metrics) {\r\n  umap_embedding <- normalized_features %>%\r\n  select(c(12:23)) %>%\r\n  umap(n_neighbors = nn,min_dist = md,metric = metric, fast_sgd = TRUE)\r\n  spotify_songs_emb <- spotify_songs_emb %>% \r\n  bind_cols(umap_embedding[,1]%>% as_tibble() ) %>% \r\n  bind_cols(umap_embedding[,2] %>% as_tibble() )\r\n  names(spotify_songs_emb)[names(spotify_songs_emb) == 'value' ] = paste('nn_',nn,'md_',md,'metric',metric,'1',sep = '.')\r\n  names(spotify_songs_emb)[names(spotify_songs_emb) == 'value1' ] = paste('nn_',nn,'md_',md,'metric',metric,'2',sep = '.')\r\n  }\r\n }\r\n \r\n}\r\n\r\nJust like what I did for T-SNE, I will focus on the same list of artists.\r\n\r\n\r\nShow code\r\nspotify_songs_emb <- spotify_songs_emb%>% \r\n mutate(\r\n  selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), \"\"),\r\n  point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),\r\n  track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),\r\n  genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),\r\n  popular_tracks_selected_artist = if_else(\r\n   track_artist %in% selected_artists & track_popularity > 65,shorter_names, NULL )) %>%\r\n distinct(track_name, .keep_all = TRUE)\r\n\r\nNow, it was time to plot the results of UMAP embeddings using ggplot and gghighlight.\r\nSetting 1\r\nNearest neighbors: 50\r\nMinimum distance: 0.09\r\nDistance metric: Euclidean\r\n\r\n\r\nShow code\r\nspotify_songs_emb %>%\r\n ggplot(aes(x = nn_.50.md_.0.09.metric.euclidean.1, y = nn_.50.md_.0.09.metric.euclidean.2 ,color = selected_artist )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size=0.8, color = '#FFE66D')) +\r\n scale_color_manual(values = c('#5BC0EB','#FDE74C','#7FB800','#E55934','#FA7921','#1A936F' ,'#F0A6CA','#B8BEDD'))+\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    color = '') \r\n\r\n\r\nSetting 2\r\nNearest neighbors: 50\r\nMinimum distance: 0.09\r\nDistance metric: Hamming\r\n\r\n\r\nShow code\r\nspotify_songs_emb %>%\r\n ggplot(aes(x = nn_.50.md_.0.09.metric.hamming.1, y = nn_.50.md_.0.09.metric.hamming.2,color = selected_artist )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size=0.8, color = '#FFE66D')) +\r\n scale_color_manual(values = c('#5BC0EB','#FDE74C','#7FB800','#E55934','#FA7921','#1A936F' ,'#F0A6CA','#B8BEDD'))+\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    color = '') \r\n\r\n\r\nSetting 3\r\nNearest neighbors: 150\r\nMinimum distance: 0.09\r\nDistance metric: Euclidean\r\n\r\n\r\nShow code\r\nspotify_songs_emb %>%\r\n ggplot(aes(x = nn_.150.md_.0.09.metric.euclidean.1, y = nn_.150.md_.0.09.metric.euclidean.2,color = playlist_genre )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size=0.8, color = '#FFE66D')) +\r\n scale_color_tableau() +\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    color = '') \r\n\r\n\r\nSetting 4\r\nNearest neighbors: 15\r\nMinimum distance: 0.09\r\nDistance metric: Euclidean\r\n\r\n\r\nShow code\r\nspotify_songs_emb %>%\r\n ggplot(aes(x = nn_.15.md_.0.09.metric.euclidean.1, y = nn_.15.md_.0.09.metric.euclidean.2,color = playlist_genre )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size=0.8, color = '#FFE66D')) +\r\n scale_color_tableau() +\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    color = '') \r\n\r\n\r\nSetting 5\r\nNearest neighbors: 150\r\nMinimum distance: 0.001\r\nDistance metric: Euclidean\r\n\r\n\r\nShow code\r\nspotify_songs_emb %>%\r\n ggplot(aes(x = nn_.150.md_.0.001.metric.euclidean.1, y = nn_.150.md_.0.001.metric.euclidean.2,color = playlist_genre )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size=0.8, color = '#FFE66D')) +\r\n scale_color_tableau() +\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    color = '') \r\n\r\n\r\nSetting 6\r\nNearest neighbors: 15\r\nMinimum distance: 0.09\r\nDistance metric: Hamming\r\n\r\n\r\nShow code\r\nspotify_songs_emb %>%\r\n ggplot(aes(x = nn_.15.md_.0.09.metric.hamming.1, y = nn_.15.md_.0.09.metric.hamming.2,color = playlist_genre )) +\r\n geom_point(size = 5.3,alpha =0.8) +\r\n gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size=0.8, color = '#FFE66D')) +\r\n scale_color_tableau() +\r\n guides(size = FALSE,\r\n  color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n  geom_text_repel(aes(label = popular_tracks_selected_artist),size = 8, family = 'Montserrat',\r\n  point.padding = 2.2,\r\n  box.padding = .5,\r\n  force = 1,\r\n  min.segment.length = 0.1) +\r\n labs(x = \"\", y = \"\" ,\r\n    color = '') \r\n\r\n\r\nFor the most part, both t-SNE and UMAP place songs from the same artists or similar songs close to each other. The UMAP embeddings with Euclidean distance are somehow similar to a real map. In the UMAP representation of the songs, we can see isolated clusters of songs. However, in t-SNE representation, no clear and separate cluster of points can be seen. We can observe that the most influential hyper-parameter seems to be the distance metric. Additionally, when we decrease the value of min_dist, the projection becomes less compact, and the global structure emerges. However, we also see that sometimes music genres are not well-separated as we would like them to be. We should take into account that audio features might not be enough to distinguish between genres of music, and We need to incorporate other aspects of songs such as lyrics to differentiate between genres. For instance, Kaylin Pavlik, in her blogpost explained how she based on similar audio features, trained several machine learning models to classify songs into six main categories (EDM, Latin, Pop, R&B, Rap, & Rock). Her best model achieved an accuracy of 54.3%, which is a decent performance but not super accurate. I also tuned and trained a few machine learning models on this dataset, but I could not achieve higher performance.\r\nSupervised UMAP\r\nUMAP is an unsupervised dimensionality reduction algorithm, but we can also feed target labels to UMAP and make it a supervised algorithm by specifying the target variable. To make this happen in UWOT, we can give the target column (playlist_genre) as an input to y argument.\r\n\r\n\r\nShow code\r\nsupervised_umap_embedding_df <- \r\n  spotify_songs %>% \r\n  select(-c(12:22)) %>% \r\n  bind_cols(supervised_umap_embedding %>% as_tibble()) %>% \r\n  dplyr::rename(umap_1 = V1, umap_2 = V2) %>% \r\n  mutate(\r\n    selected_artist = if_else( track_artist %in% selected_artists, as.character(track_artist), \"\"),\r\n    point_size_selected_artist = if_else(track_artist %in% selected_artists, 0.5, 0.1),\r\n    track_name_selected_artist = if_else(track_artist %in% selected_artists, track_name, NULL),\r\n    genre_selected_artist = if_else(track_artist %in% selected_artists,playlist_genre, NULL),\r\n    popular_tracks_selected_artist = if_else(\r\n      track_artist %in% selected_artists & track_popularity > 70,shorter_names, NULL )) %>%\r\n  distinct(track_name, .keep_all = TRUE)\r\n\r\n\r\n\r\nShow code\r\nsupervised_umap_embedding_df %>%\r\n  ggplot(aes(x = umap_1, y = umap_2 ,color = playlist_genre )) +\r\n  geom_point(size = 5.3,alpha =0.8 ) +\r\n  gghighlight(selected_artist != \"\",unhighlighted_params = list(alpha = 0.3,size = 0.8, color = '#FFE66D')) +\r\n  scale_color_tableau() +\r\n  guides(size = FALSE,\r\n    color = guide_legend(override.aes = list(alpha = 0.9,size = 12))) +\r\n    geom_text_repel(aes(label = popular_tracks_selected_artist),size = 7, family = 'Montserrat',\r\n    point.padding = 2.2,\r\n    box.padding = .5,\r\n    force = 1,\r\n    min.segment.length = 0.1) +\r\n  labs(x = \"\", y = \"\" ,\r\n       color = '') \r\n\r\n\r\nIt is no surprise that the results of the supervised UMAP are much better separated than the unsupervised one. We just gave additional information to UMAP to transform input data.\r\n\r\n\r\n",
    "preview": "posts/2020-02-01-spoitfymaps/featured.JPG",
    "last_modified": "2021-06-06T23:48:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-31-explainable-data-science-summer-school/",
    "title": "Explainable Data Science Summer School",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Muhammad Chenariyan Nakhaee",
        "url": {}
      }
    ],
    "date": "2019-12-31",
    "categories": [
      "XAI - Data Science"
    ],
    "contents": "\r\n\r\nContents\r\nFrom Data Mining to Data Science - Peter Flach (EuADS President)\r\n1. What is Data Science?**\r\n2. From Data Mining to Data Science\r\n3. Responsible Data Science - The Human Factor (3/5)\r\n\r\nModel-Based Machine Learning\r\n\r\nLast September, I had the opportunity to participate in the EXPLAINABLE DATA SCIENCE summer school in Kirchberg, Luxembourg. the summer school was organized by the European Association for Data Science (EuADS) and was held during 10-13 September.\r\nWhat I specifically liked about this summer school ( of course besides enjoying the the beautiful city of Luxembourg ) was the fact that it covered a vast variety of topics in the explainable machine learning (AI) literature, ranging from visualization, XAI techniques, causality to psychological aspects of explainability. In addition, the summer school has a special guest, the legendary Christopher Bishop who gave the inaugural lecture.\r\nYou can find the complete program and the presentations in the EuADS’s website. Nevertheless during some presentations in the summer school, I took notes and I summarized them.\r\n\r\nDisclaimer:\r\nSometimes it is not easy to keep up with the speaker and take notes. Also, it is possible that what I wrote down is just my interpretation and not what the speaker intened to say. For this reason, I do not guarantee that all details in this post are accurate or what the speakers wanted to communicate.\r\n\r\nFrom Data Mining to Data Science - Peter Flach (EuADS President)\r\n1. What is Data Science?**\r\n“Data Science” is a vague term. One might mean by “data science”:\r\nIt is the Science of data. This definition is more frequently used by statistician and machine learning and is more theoretical.\r\nDoing science with data. This definition is more applied and data intensive.\r\nApplying science to data. This definition is also heavily applied and data intensive.\r\nData is not the New Oil\r\nSome people are overexcited about having access to huge amount of data as if they have discovered an oil field. Likewise, they believe that they can simply extract value from data and this data is a new driver for progress and prosperity. However, even if we acquire data we cannot be certain that it is valuable and we can extract value from it.\r\nIn other words, data in and of itself does not present value:\r\ndata != value but\r\nBut data and knowledge together can result in value. Here knowledge can be an input or an output of the data.\r\ndata + knowledge = value\r\nNow data science can defined as follows:\r\n1570983727831It means that Data Science has three main ingredients:\r\n Data \r\n Knowledge\r\n Value \r\nThe kinds of value that Data Science can generate are:\r\nscientific knowledge and models\r\nsocietal value\r\neconomic value\r\npersonal value\r\n2. From Data Mining to Data Science\r\nMany consider data mining to be the father of data science. Others say that data mining is a subset of data science. While the interest for data mining is declining, data science gain more popularity.\r\ntestFig. 2: Data science is getting more popular than data mining\r\ntestFig. 3: CRISP data mining process\r\nIn data mining, we (implicitly) assume that there is some value in the and our aim is to use data mining techniques to uncover it. We can see data mining just like the extraction of a valuable metals from an existing mine.\r\nHowever, in data science, we first need to make sure that data has some value. In other words, data science can be seen as prospective, which means we are searching for a mine to extract metal material from it. That puts more emphasis on the exploratory aspect (nature) of data science, which includes the following activities:\r\n1571180675528These activities do not exist in the data mining space and distinguish data science and data mining.\r\n\r\nData Science Trajectory (DST) space\r\nData mining is a more sequential and more prescriptive approach where every operation must be implemented in a specific order. All activities in data mining can be a part of a data science project but not the opposite. For instance, not every data science project requires a modeling phase. On the other hand, the goal of data science for a specific application can be just data collection or data publication.\r\nimage-20200104194252866image-20200104194422595Read more about this in the following paper:\r\n\r\nCRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories. Fernando Martinez-Plumed, Lidia Contreras-Ochando, Cesar Ferri, Jose Hernandez-Orallo, Meelis Kull, NicolasLachiche, Maria Jose Ramirez-Quintana and Peter Flach. (Under review, 2019)\r\n\r\n3. Responsible Data Science - The Human Factor (3/5)\r\nData Science is for, about, by and with humans and human factors should be taken into consideration at every stage of a data science project. But it is not always easy to measure, define and ultimately achieve them.\r\nFor example, look at following table which shows the number and the percentage of students who applied and were admitted to a university.\r\n1571181577381At the first glance, this table might suggest a case of bias toward women in the admission process. However, further examinations show that the low percentage of total admissions for women is due to the fact that female applicants tended to apply to more difficult programs with an overall lower chance of acceptance while men applied to easier programs with a higher probability of acceptance. In other words, the difficulty of programs was a confounding factor that influenced the outcome not gender bias. It indicates measuring a human factor such as fairness is not easy because measuring bias is not easy. Furthermore, according to Goodhart’s Law, the moment we decide to use these metrics (e.g. bias) as our target to optimize, they are not good measures anymore.\r\nin the the rest of talk, Peter Flach discussed the relationship between GDPR and fairness and specifically he touched upon an important issue regarding data ownership and the role of GDPR for personal data protection.\r\n1571585896393He provided an example of authorship to demonstrate that solving data ownership is not a simple task. If someone writes a book about someone else (e.g. Clinton), the author has the ownership and the copyright not the the person whom the book is about.\r\nResources:\r\nSlides\r\nModel-Based Machine Learning\r\n\r\nThis talk was dedicated to Sabine Krolak-Schwerdt who unfortunately passed away recently and was one of the founders of EuADS.\r\n\r\nThree factors have contributed to the popularity and the recent success of AI\r\nMore computing power\r\nLarge amount of available data (Big data)\r\nMore powerful algorithms\r\nDozens of machine learning algorithms have been developed.\r\nimage-20200104221438797But the ‘No Free Lunch Theorem’ states that no universal machine learning can solve every problem.\r\n\r\nAveraged over all possible data distributions, every classification algorithm has the same error rate when classifying previously unobserved points. D. Wolpert (1996)\r\n\r\nThis means that the goal of machine learning is to find an algorithm that is well-suited to the problem that being solved.\r\nModel-Based machine learning\r\nIn the traditional machine learning paradigm, ML algorithms play a centric role. We start by an ML algorithm and we would like to know how we can apply it to our problem.\r\nHowever, in model-based machine learning paradigm, we are looking to find a well-matched algorithm for our problem. We can derive a model that best represents our problem by making explicit modeling assumptions.\r\nData and prior knowledge\r\nScenario 1: we have collected a handful of voltage and current measurement from an experiment. and we want to determine the relationship between the current and voltage using these measurements.\r\nScenario 2: We have a huge database containing images from 1000 objects and our goal is to develop a model to classify each image correctly.\r\n1571589636558But are these datasets ‘big’ enough for solving their corresponding problems. In the first scenario, although we only have a few measurements, we know that they are enough for finding the relationship between voltage and current. On the other hand, even though we have access to a large number of images for each class, these images do not represent the distribution of all images.\r\n\r\n1571589623467The trade-off between prior knowledge and the amount of data needed\r\nTherefore, we must distinguish between two types of ‘big data’:\r\nIn terms of size\r\nIn terms of being statistically significant\r\nThen, Chris Bishop argued that we need to incorporate uncertainties into our machine learning models otherwise the consequences would be dire. It means that we should never ever build direct classifier but we should build probabilistic classifier.\r\n1571590428222Why is that? Because not all misclassification errors are equal and different costs are assigned to different errors. Misclassifying a patient with cancer may be much worse than misclassifying a healthy patient. So, instead of minimizing the number of misclassified instances, we can minimize the expected (average costs).\r\nFinally, Chris Bishop presented a demo of a movie recommendation system.\r\nResources:\r\nSlides\r\nModel-Based Machine Learning(\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T14:47:11+02:00",
    "input_file": {}
  }
]
